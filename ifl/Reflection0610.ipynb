{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM71ZKtG8mK9I0hJol60Z18",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/workspace/blob/main/ifl/Reflection0610.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wordcloud with Korean text"
      ],
      "metadata": {
        "id": "4MhPM0nRuTyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[text](https://raw.githubusercontent.com/MK316/workspace/main/ifl23/data/reflection.csv) in csv file"
      ],
      "metadata": {
        "id": "Cblfh2zzvndN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "sHXU93NjwbrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file from GitHub\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/MK316/workspace/main/ifl23/data/reflection.csv', encoding='utf-8')"
      ],
      "metadata": {
        "id": "cHvIZMHG4fUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iPyzfEAByM6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppn-NfO6uSti"
      },
      "outputs": [],
      "source": [
        "# Combine all rows in the 'reflection' column into one string\n",
        "# text = ' '.join(df['Reflection'])\n",
        "text = ' '.join(df['RE'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# You'll need to have a list of Korean stop words available\n",
        "stop_words = [\"n't\", 'think',\"thing\",\"I\",\"class\",\"classes\",\"taking\",\"course\",\"used\",\"using\"]  # replace this with your actual list of Korean stop words\n",
        "\n",
        "# Remove stop words\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Create a WordCloud\n",
        "wordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(tokens))\n",
        "\n",
        "# wordcloud = WordCloud(\n",
        "#     font_path='/content/your_font_file.ttf',  # replace with the name of your uploaded font file\n",
        "#     width = 1000, \n",
        "#     height = 500\n",
        "# ).generate(' '.join(tokens))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.text import Text\n",
        "\n",
        "# Let's say you want to extract the context of the word \"example\" in the 'reflection' column of your DataFrame\n",
        "word = 'never'\n",
        "\n",
        "# Combine all rows in the 'reflection' column into one string\n",
        "text = ' '.join(df['RE'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Create a Text object\n",
        "text_obj = Text(tokens)\n",
        "\n",
        "# Create a concordance (KWIC) for the word\n",
        "text_obj.concordance(word)\n"
      ],
      "metadata": {
        "id": "u-eMmUKQ8f64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "C3-TuBg141Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment analysis\n",
        "# Please note that SentimentIntensityAnalyzer is designed for English text, so it may not give accurate results for Korean text\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "print(sentiment_scores)"
      ],
      "metadata": {
        "id": "X8nkUtGuxFUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result description**: The sentiment analysis of the text reveals a predominantly neutral tone, with 81.1% of the content classified as such. Positive sentiment constitutes 14.7% of the text, while negative sentiment is minimal, accounting for only 4.2%. Most notably, the compound score, a metric that encapsulates the overall sentiment of the text, is extremely high at 0.9999 (on a scale from -1 to 1). This suggests that despite the considerable proportion of neutral content, the text as a whole leans heavily towards a positive sentiment.\n",
        "\n",
        "+ 'neg': This is the Negative sentiment score. It represents the proportion of the text that is classified as negative. In your case, 4.2% of the text is classified as negative.\n",
        "\n",
        "+ 'neu': This is the Neutral sentiment score. It represents the proportion of the text that is classified as neutral. In your case, 81.1% of the text is classified as neutral.\n",
        "\n",
        "+ 'pos': This is the Positive sentiment score. It represents the proportion of the text that is classified as positive. In your case, 14.7% of the text is classified as positive.\n",
        "\n",
        "+ 'compound': This is the Compound score. It is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1 (most extreme negative) and +1 (most extreme positive). This score is a good indicator of the overall sentiment of the text. Positive values indicate positive sentiment, negative values indicate negative sentiment, and values close to zero indicate neutral sentiment. In your case, the compound score is 0.9999, which is very close to 1, indicating a very strong positive sentiment overall.\n",
        "+ The compound score in the NLTK's SentimentIntensityAnalyzer is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1 (most extreme negative) and +1 (most extreme positive).\n",
        "\n",
        "The calculation of the compound score is a bit complex. It uses the valence scores of each word in the text, which are determined by the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tool. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n",
        "\n",
        "Here is a simplified explanation of how it works:\n",
        "\n",
        "Each word in the text is assigned a valence score from the VADER lexicon. The valence score of a word is a measure of its emotional intensity ranging from -4 to +4. For example, the word \"love\" has a valence score of 3.2.\n",
        "\n",
        "These scores are adjusted based on the context of the word in the text. For example, if a word is in ALL CAPS, its valence score is increased. If a word is preceded by a negation word (like \"not\" or \"never\"), its valence score is reversed.\n",
        "\n",
        "The adjusted scores are then summed up and normalized to be between -1 and +1 to get the compound score.\n",
        "\n",
        "The exact calculation involves some additional steps and is beyond the scope of this explanation. If you're interested in the details, you can check out the source code of the SentimentIntensityAnalyzer or the original paper on VADER.\n",
        "\n",
        "Please note that the compound score is a good indicator of the overall sentiment of the text, but it may not always accurately reflect the sentiment of individual sentences or phrases within the text."
      ],
      "metadata": {
        "id": "QCudxDbF5x5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sentiment analysis\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(sentiment_scores.keys(), sentiment_scores.values())\n",
        "plt.title('Sentiment Scores')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WybKmBPZ5How"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.pie(sentiment_scores.values(), labels=sentiment_scores.keys(), autopct='%1.1f%%')\n",
        "plt.title('Sentiment Scores')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kfAvuAbV6CxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.barh(list(sentiment_scores.keys()), list(sentiment_scores.values()))\n",
        "plt.title('Sentiment Scores')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "16zhBb986JnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential colormaps: These are good for data that ranges from low to high. Examples include 'Blues', 'BuGn' (blue to green), 'OrRd' (orange to red), and 'Greys'.\n",
        "\n",
        "Diverging colormaps: These are good for data that has a meaningful zero point or a specific value of interest. Examples include 'coolwarm', 'bwr' (blue, white, red), 'seismic', and 'RdBu' (red to blue).\n",
        "\n",
        "Qualitative colormaps: These are good for data that has no inherent ordering, such as categories or labels. Examples include 'Pastel1', 'Set1', 'tab10', and 'tab20'.\n",
        "\n",
        "Miscellaneous colormaps: These include 'flag', 'prism', 'ocean', 'gist_earth', 'terrain', and 'nipy_spectral'."
      ],
      "metadata": {
        "id": "9Qyi0dGL6rFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Convert the scores to a DataFrame\n",
        "df_scores = pd.DataFrame([sentiment_scores])\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(df_scores, annot=True, cmap='RdBu')\n",
        "plt.title('Sentiment Scores')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3PXH1IEi6Lbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword analysis"
      ],
      "metadata": {
        "id": "Pi3-ptVj8eVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, row in df.iterrows():\n",
        "    text = ' '.join(df['RE'].dropna().astype(str))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    text_obj = Text(tokens)\n",
        "    text_obj.concordance(word)\n"
      ],
      "metadata": {
        "id": "KfP7lTem8zar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.text import Text\n",
        "\n",
        "def limited_concordance(txt_obj, keyword, width=79, lines=25):\n",
        "    \"\"\"\n",
        "    Print a limited concordance for `keyword` in `txt_obj`.\n",
        "\n",
        "    :param txt_obj: The Text object to search.\n",
        "    :param keyword: The word to find.\n",
        "    :param width: The width of each line (default is 79, same as nltk.Text.concordance).\n",
        "    :param lines: The maximum number of lines to print (default is 25, same as nltk.Text.concordance).\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a concordance index\n",
        "    concordance_index = nltk.text.ConcordanceIndex(txt_obj.tokens)\n",
        "\n",
        "    # Find the offsets for the keyword\n",
        "    offsets = concordance_index.offsets(keyword)\n",
        "\n",
        "    # Only keep the first `lines` offsets\n",
        "    offsets = offsets[:lines]\n",
        "\n",
        "    # Print the concordance\n",
        "    for offset in offsets:\n",
        "        left = ' '.join(txt_obj.tokens[offset-width//2 : offset])\n",
        "        right = ' '.join(txt_obj.tokens[offset+1 : offset+width//2+1])\n",
        "        print(f'{left[-width//2:]:{width//2}} {txt_obj.tokens[offset]} {right[:width//2]:{width//2}}')\n",
        "\n",
        "# Combine all rows in the 'reflection' column into one string\n",
        "text = ' '.join(df['RE'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Create a Text object\n",
        "text_obj = Text(tokens)\n",
        "\n",
        "# Create a limited concordance for the word \"example\"\n",
        "limited_concordance(text_obj, 'coding', lines=10)"
      ],
      "metadata": {
        "id": "NSfDA6S09di1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modal verbs"
      ],
      "metadata": {
        "id": "ayjH91J4_Z9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Define your string\n",
        "# text = \"I can play the guitar. He should go to the doctor. They might arrive late.\"\n",
        "text = ' '.join(df['RE'].dropna().astype(str))\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Define a list of modal verbs\n",
        "modal_verbs = ['can', 'could', 'may', 'might', 'will', 'would', 'shall', 'should', 'must']\n",
        "\n",
        "# Use a list comprehension to extract modal verbs\n",
        "modals = [word for word in tokens if word in modal_verbs]\n",
        "\n",
        "print(modals)\n"
      ],
      "metadata": {
        "id": "oYeMgrQE_bhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Define your string\n",
        "text = ' '.join(df['RE'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Define a list of modal verbs\n",
        "modal_verbs = ['can', 'could', 'may', 'might', 'will', 'would', 'shall', 'should', 'must']\n",
        "\n",
        "# Use a list comprehension to extract modal verbs\n",
        "modals = [word for word in tokens if word in modal_verbs]\n",
        "\n",
        "# Count the frequency of each modal verb\n",
        "counter = Counter(modals)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(counter.keys(), counter.values())\n",
        "plt.xlabel('Modal Verbs')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency of Modal Verbs')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rJ2yJcHr_rlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network analysis"
      ],
      "metadata": {
        "id": "aLFTI6ErBwXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['RE'].dropna().astype(str))\n",
        "\n",
        "# Preprocessing and tokenization\n",
        "tokens = text.split()  # Split text into tokens\n",
        "\n",
        "# Build the network\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(tokens)  # Add nodes for each token\n",
        "\n",
        "# Add edges based on relationships (e.g., co-occurrence)\n",
        "for i in range(len(tokens) - 1):\n",
        "    G.add_edge(tokens[i], tokens[i+1])\n",
        "\n",
        "# Visualize the network graph\n",
        "pos = nx.spring_layout(G)  # Position nodes using a spring layout algorithm\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
        "plt.title(\"Network Graph\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l__8c-AKByd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['RE'].dropna().astype(str))\n",
        "\n",
        "# Preprocessing and tokenization\n",
        "tokens = text.split()  # Split text into tokens\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 30  # Adjust the threshold as needed\n",
        "\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Build the network graph with high-frequency words\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(high_freq_words)  # Add nodes for each high-frequency word\n",
        "\n",
        "# Add edges based on relationships (e.g., co-occurrence)\n",
        "for i in range(len(tokens) - 1):\n",
        "    if tokens[i] in high_freq_words and tokens[i+1] in high_freq_words:\n",
        "        G.add_edge(tokens[i], tokens[i+1])\n",
        "\n",
        "# Visualize the network graph\n",
        "pos = nx.spring_layout(G)  # Position nodes using a spring layout algorithm\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
        "plt.title(\"Network Graph (High-Frequency Words)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Q1VrIpc0CG3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['RE'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 10  # Adjust the threshold as needed\n",
        "\n",
        "# Get the high-frequency words\n",
        "# high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(filtered_tokens)  # Add nodes for each filtered token\n",
        "\n",
        "# Add edges based on relationships (e.g., co-occurrence)\n",
        "for i in range(len(filtered_tokens) - 1):\n",
        "    G.add_edge(filtered_tokens[i], filtered_tokens[i+1])\n",
        "\n",
        "# Visualize the network graph\n",
        "pos = nx.spring_layout(G)  # Position nodes using a spring layout algorithm\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
        "plt.title(\"Network Graph (Without Stopwords)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l5VhY4zzCkpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['RE'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove punctuations and \"n't\"\n",
        "tokens = [word for word in tokens if word not in string.punctuation and word not in [\"n't\", \"think\", \"thought\",\"used\",\"using\",\"class\",\"classes\",\"course\",\"taking\",\"'s\",\"things\"]]\n",
        "\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 5  # Adjust the threshold as needed\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(filtered_tokens)  # Add nodes for each filtered token\n",
        "\n",
        "# Add edges based on relationships (e.g., co-occurrence)\n",
        "for i in range(len(filtered_tokens) - 1):\n",
        "    G.add_edge(filtered_tokens[i], filtered_tokens[i+1])\n",
        "\n",
        "plt.figure(figsize=(12, 6))  # Adjust the width and height as desired\n",
        "\n",
        "\n",
        "# Visualize the network graph\n",
        "pos = nx.spring_layout(G)  # Position nodes using a spring layout algorithm\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
        "plt.title(\"Network Graph (Without Stopwords)\")\n",
        "plt.savefig('network_graph.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L4sCI5YyEnCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a network plot, connected words represent relationships or connections between those words based on their co-occurrence or proximity in the text. The connections can provide insights into how words are related to each other and how they appear together in the given context.\n",
        "\n",
        "Here are some interpretations of the network plot in terms of neighboring and distancing:\n",
        "\n",
        "Neighboring Words: Nodes that are closely connected or appear in close proximity to each other in the network plot indicate a higher likelihood of co-occurrence in the text. This suggests that these words are often found together or have a strong association within the context of the data.\n",
        "\n",
        "Distancing Words: Nodes that are farther apart in the network plot indicate a lower likelihood of co-occurrence or association in the text. These words may have less frequent or weaker connections, suggesting they are less related to each other within the context.\n",
        "\n",
        "Clusters or Communities: Within the network plot, you may observe clusters or groups of nodes that are tightly connected among themselves but have fewer connections with nodes in other clusters. These clusters can represent semantic or thematic groups of words that share common characteristics or have similar contexts.\n",
        "\n",
        "Central Nodes: Some nodes in the network may act as central or highly connected nodes, with many edges connecting to them. These central nodes are often important keywords or concepts that are more prevalent or influential within the given text.\n",
        "\n",
        "It's important to note that the interpretation of the network plot should be done in conjunction with the specific context and data being analyzed. Different network structures and patterns may emerge based on the characteristics of the text or the specific domain of study."
      ],
      "metadata": {
        "id": "tdJNmYIRFo0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvis"
      ],
      "metadata": {
        "id": "R1qL3sFaJj6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "from pyvis.network import Network\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['RE'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text into sentences and then into words\n",
        "sentences = sent_tokenize(text)\n",
        "tokens = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Remove punctuations and specific words\n",
        "tokens = [[word for word in sent if word not in string.punctuation and word not in [\"n't\", \"think\", \"thought\",\"used\",\"using\",\"class\",\"classes\",\"course\",\"taking\",\"'s\",\"things\"]] for sent in tokens]\n",
        "\n",
        "# Flatten the list of sentences into a list of words\n",
        "flat_tokens = [word for sent in tokens for word in sent]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(flat_tokens)\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 5  # Adjust the threshold as needed\n",
        "\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes for each filtered token\n",
        "G.add_nodes_from(filtered_tokens)\n",
        "\n",
        "# Add edges based on co-occurrence within the same sentence\n",
        "for sent in tokens:\n",
        "    for i in range(len(sent) - 1):\n",
        "        if sent[i] in filtered_tokens and sent[i+1] in filtered_tokens:\n",
        "            G.add_edge(sent[i], sent[i+1])\n",
        "\n",
        "# Create a Network instance\n",
        "nt = Network(notebook=True, height=\"500px\", width=\"100%\", cdn_resources='remote')\n",
        "\n",
        "# Add nodes and edges to the network\n",
        "for node in G.nodes:\n",
        "    nt.add_node(node)\n",
        "for edge in G.edges:\n",
        "    nt.add_edge(edge[0], edge[1])\n",
        "\n",
        "# Visualize the network graph\n",
        "nt.show(\"network_graph.html\")\n"
      ],
      "metadata": {
        "id": "Xm3G86OzJg5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling\n",
        "\n"
      ],
      "metadata": {
        "id": "-LQQhgINL4gY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gensim"
      ],
      "metadata": {
        "id": "NAuFll9aMG_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim import corpora, models\n",
        "import pandas as pd\n",
        "\n",
        "# Combine all rows in the 'RE' column into one string\n",
        "text = ' '.join(df['RE'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# List of stop words\n",
        "stop_words = [\"n't\", 'think',\"thing\",\"I\",\"class\",\"classes\",\"taking\",\"course\",\"used\",\"using\"]\n",
        "\n",
        "# Remove stop words\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Create a dictionary from the tokens\n",
        "dictionary = corpora.Dictionary([tokens])\n",
        "\n",
        "# Create a corpus from the dictionary\n",
        "corpus = [dictionary.doc2bow([token]) for token in tokens]\n",
        "\n",
        "# Initialize an LDA model\n",
        "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
        "\n",
        "# Print the topics\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "metadata": {
        "id": "Log0lpOvL7Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# You can add your custom stopwords to this set\n",
        "custom_stopwords = [\"n't\", 'think',\"thing\",\"I\",\"class\",\"classes\",\"taking\",\"course\",\"used\",\"using\",\"us\",\"'s\",'``']\n",
        "stop_words.update(custom_stopwords)\n"
      ],
      "metadata": {
        "id": "BxxckFkmMJKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim import corpora, models\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "# Combine all rows in the 'RE' column into one string\n",
        "text = ' '.join(df['RE'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Add custom stopwords\n",
        "custom_stopwords = [\"n't\", 'think',\"thing\",\"I\",\"class\",\"classes\",\"taking\",\"course\",\"used\",\"using\"]\n",
        "stop_words.update(custom_stopwords)\n",
        "\n",
        "# Remove stop words and punctuation\n",
        "tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        "\n",
        "# Create a dictionary from the tokens\n",
        "dictionary = corpora.Dictionary([tokens])\n",
        "\n",
        "# Create a corpus from the dictionary\n",
        "corpus = [dictionary.doc2bow([token]) for token in tokens]\n",
        "\n",
        "# Initialize an LDA model\n",
        "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
        "\n",
        "# Print the topics\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "metadata": {
        "id": "620TdtmkMhjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output you're seeing is the result of the LDA model's topic extraction. Each line represents a topic, and the numbers next to each word represent the weight of that word in the topic. The higher the weight, the more important the word is to that topic.\n",
        "\n",
        "Here's a brief interpretation of your results:\n",
        "\n",
        "Topic 0 seems to be about coding and education, with words like \"coding\", \"teaching\", \"education\", \"Literacy\", \"familiar\".\n",
        "Topic 1 could be about future prospects or plans for teachers, with words like \".\", \"would\", \"teachers\", \"future\", \"able\".\n",
        "Topic 2 might be about the need to utilize digital tools or resources, with words like \",\", \"digital\", \"become\", \"need\", \"utilize\".\n",
        "Topic 3 seems to be about thoughts and reflections on various aspects, with words like \"like\", \"various\", \"thought\", \"time\", \"If\".\n",
        "Topic 4 could be about resources and media used in teaching, with words like \"materials\", \"feel\", \"competencies\", \"media\", \"videos\".\n",
        "Remember, these are just interpretations based on the top words in each topic. The actual meaning of the topics can depend on the context of your data."
      ],
      "metadata": {
        "id": "KKwS2e3mM42F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For each document in the corpus\n",
        "for i, row in enumerate(corpus):\n",
        "    # Get the topic distribution for the document\n",
        "    doc_topics = lda_model.get_document_topics(row)\n",
        "    \n",
        "    # Print the document ID and the topic distribution\n",
        "    # print(f\"Document {i}: {doc_topics}\")\n"
      ],
      "metadata": {
        "id": "qcIpsD2qM4RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each document in the corpus\n",
        "for i, row in enumerate(corpus):\n",
        "    # If i is 10 or more, break the loop\n",
        "    if i >= 10:\n",
        "        break\n",
        "\n",
        "    # Get the topic distribution for the document\n",
        "    doc_topics = lda_model.get_document_topics(row)\n",
        "    \n",
        "    # Print the document ID and the topic distribution\n",
        "    print(f\"Document {i}: {doc_topics}\")\n"
      ],
      "metadata": {
        "id": "RaKLulgzNT3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "DN0ZSlbhNkF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "# Prepare the visualization\n",
        "vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "\n",
        "# Display the visualization\n",
        "pyLDAvis.display(vis_data)\n"
      ],
      "metadata": {
        "id": "BO9vP7aCNWZ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}