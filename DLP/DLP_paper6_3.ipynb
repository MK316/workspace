{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMo8zuXuB4RCAFvbiGbQ2Lb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/workspace/blob/main/DLP/DLP_paper6_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Digital Literacy paper:\n",
        "\n",
        "Asof 1119"
      ],
      "metadata": {
        "id": "iSZeS_upqLF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] File to read"
      ],
      "metadata": {
        "id": "vzaoP8SYqU4j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P0PrjfiqGv2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/DLtotal_1116.csv\")\n",
        "df[-1:]\n",
        "\n",
        "info = pd.read_csv(\"/content/DLtotal_1116_heads.csv\")\n",
        "info[-1:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the column names as a list\n",
        "column_names = list(df.columns)\n",
        "\n",
        "print(column_names)"
      ],
      "metadata": {
        "id": "4fo9oppskTYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2] Data stats: descriptive statistics\n",
        "\n",
        "Major in percentage: The survey sample included 228 students enrolled in English major courses at both undergraduate and graduate levels. Among them, 52.6% specialized in various sub-fields of English Language studies, 38.6% in English Education, and 8.8% in other areas. The undergraduate group formed the majority at 86.8% (N=198), with first-year students comprising 34.21% (N=78), second years 28.51% (N=65), third years 13.16% (N=30), and fourth years 10.96% (N=25). Graduate students accounted for 13.2% of the sample, with 9.65% (N=22) in Master programs and 3.51% (N=8) pursuing PhDs."
      ],
      "metadata": {
        "id": "t1LEC-PcqiwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'Major' is a column in your DataFrame `df`\n",
        "major_counts = df['Major'].value_counts()\n",
        "major_percentages = df['Major'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Combine counts and percentages into a single DataFrame for a nicer display\n",
        "summary_df = pd.DataFrame({\n",
        "    'Count': major_counts,\n",
        "    'Percentage': major_percentages\n",
        "})\n",
        "\n",
        "# Display the summary DataFrame\n",
        "print(summary_df)\n"
      ],
      "metadata": {
        "id": "55ecjCkDrLZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Calculate counts for combinations of 'Major' and 'Level'\n",
        "grouped_counts = df.groupby(['Major', 'Level']).size()\n",
        "\n",
        "# Calculate percentages based on the grouped counts\n",
        "grouped_percentages = grouped_counts / grouped_counts.sum() * 100\n",
        "\n",
        "# Combine counts and percentages into a single DataFrame\n",
        "summary_df = pd.DataFrame({\n",
        "    'Count': grouped_counts,\n",
        "    'Percentage': grouped_percentages\n",
        "})\n",
        "\n",
        "# Reset the index if you want 'Major' and 'Level' as columns rather than a multi-index\n",
        "summary_df = summary_df.reset_index()\n",
        "\n",
        "# Display the summary DataFrame\n",
        "print(summary_df)\n"
      ],
      "metadata": {
        "id": "Wg1fDalssMbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Teaching career grouping"
      ],
      "metadata": {
        "id": "77JT5p0JsmFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group comparison: prospective teachers vs. language learners"
      ],
      "metadata": {
        "id": "mrnspQDbz0C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataframe is already read and named as 'df'\n",
        "\n",
        "# Create a new column for group categorization based on 'Teaching_plan'\n",
        "df['Group'] = df['Teaching_plan'].apply(lambda x: 'Group 1-3' if x in [1, 2, 3] else 'Group 4-6')\n",
        "\n",
        "# Now, group by the newly created 'Group' column and compare the 'E1' column\n",
        "# Using agg function to get count, mean, standard deviation, and median\n",
        "grouped_data = df.groupby('Group')['E1'].agg(['count', 'mean', 'std', 'median'])\n",
        "\n",
        "# Display the comparison\n",
        "print(grouped_data)\n"
      ],
      "metadata": {
        "id": "NlTFHx91z5m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a new column with 'HTI' and 'LTI' and name the column as 'TeachingInterest', and update df"
      ],
      "metadata": {
        "id": "OZG7Jmnftq_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataframe is already read and named as 'df'\n",
        "\n",
        "# Create a new column 'TeachingInterest' based on 'Teaching_plan'\n",
        "df['TeachingInterest'] = df['Teaching_plan'].apply(lambda x: 'LTI' if x in [1, 2, 3] else 'HTI')\n",
        "\n",
        "# Now, group by the 'TeachingInterest' column and compare the 'E1' column\n",
        "# Using agg function to get count, mean, standard deviation, and median\n",
        "grouped_data = df.groupby('TeachingInterest')['E1'].agg(['count', 'mean', 'std', 'median'])\n",
        "\n",
        "# Display the comparison\n",
        "print(grouped_data)\n",
        "\n",
        "# # Write the DataFrame to a CSV file with UTF-8 encoding\n",
        "df.to_csv('updated_dataframe.csv', encoding='utf-8', index=False)\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/updated_dataframe.csv\")\n",
        "df[-1:]"
      ],
      "metadata": {
        "id": "1P0RMIgptzB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1 Self-assessment of digital literacy levels"
      ],
      "metadata": {
        "id": "x1daStKeva2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count and ratio table"
      ],
      "metadata": {
        "id": "2WzNhdR9x_eI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your DataFrame is already read and named as 'df'\n",
        "\n",
        "# Count the occurrences of each unique value in 'A4'\n",
        "value_counts = df['A4'].value_counts()\n",
        "\n",
        "# Calculate the ratio of each unique value in 'A4'\n",
        "total_count = df['A4'].count()\n",
        "value_ratios = value_counts / total_count * 100  # Ratio in percentage\n",
        "\n",
        "# Combine counts and ratios into a single DataFrame\n",
        "response_summary = pd.DataFrame({\n",
        "    'Count': value_counts,\n",
        "    'Ratio (%)': value_ratios\n",
        "})\n",
        "\n",
        "# Display the response summary as a table\n",
        "print(response_summary)\n"
      ],
      "metadata": {
        "id": "uiv0ARFAv8BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Literacy level in scale (1-6) summary"
      ],
      "metadata": {
        "id": "aW5HKSe1yXxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['E1'].describe()"
      ],
      "metadata": {
        "id": "_ofJPeK8yWiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-squared test"
      ],
      "metadata": {
        "id": "m5y-fNGXyi5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group comparison: prospective teachers vs. language learners"
      ],
      "metadata": {
        "id": "JZ_Yk-3Iyo-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataframe is already read and named as 'df'\n",
        "\n",
        "# Create a new column for group categorization based on 'Teaching_plan'\n",
        "df['Group'] = df['Teaching_plan'].apply(lambda x: 'Group 1-3' if x in [1, 2, 3] else 'Group 4-6')\n",
        "\n",
        "# Now, group by the newly created 'Group' column and compare the 'E1' column\n",
        "# Using agg function to get count, mean, standard deviation, and median\n",
        "grouped_data = df.groupby('Group')['E1'].agg(['count', 'mean', 'std', 'median'])\n",
        "\n",
        "# Display the comparison\n",
        "print(grouped_data)\n"
      ],
      "metadata": {
        "id": "piz8RGLmyo-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install scipy pandas"
      ],
      "metadata": {
        "id": "3sXD1FvS1Ohf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# Assuming your DataFrame is already loaded into variable df\n",
        "\n",
        "# Define the groups based on 'Teaching_plan' column\n",
        "group1 = df[df['Teaching_plan'].isin([1, 2, 3])]['E1']\n",
        "group2 = df[df['Teaching_plan'].isin([4, 5, 6])]['E1']\n",
        "\n",
        "# Perform Mann-Whitney U Test\n",
        "stat, p = mannwhitneyu(group1, group2)\n",
        "\n",
        "# Output the results\n",
        "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "    print('Same distribution (fail to reject H0)')\n",
        "else:\n",
        "    print('Different distribution (reject H0)')\n"
      ],
      "metadata": {
        "id": "DL6uCeQ51QZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chi-Squared: The two groups are different"
      ],
      "metadata": {
        "id": "nf8bH4Fo1hMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Step 2: Load your DataFrame here\n",
        "# df = pd.read_csv('path_to_your_file.csv')  # Example for loading a CSV file\n",
        "\n",
        "# Step 3: Group 'Teaching_plan' into two categories\n",
        "conditions = [\n",
        "    df['Teaching_plan'].isin([1, 2, 3]),\n",
        "    df['Teaching_plan'].isin([4, 5, 6])\n",
        "]\n",
        "choices = ['1-3', '4-6']\n",
        "df['Teaching_plan_group'] = np.select(conditions, choices, default='Other')\n",
        "\n",
        "# Step 4: Perform Chi-squared Test\n",
        "# Create a crosstab\n",
        "crosstab = pd.crosstab(df['A4'], df['Teaching_plan_group'])\n",
        "\n",
        "# Perform the Chi-squared test\n",
        "chi2, p, dof, expected = chi2_contingency(crosstab)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Chi-squared Statistic: {chi2}\")\n",
        "print(f\"P-value: {p}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(f\"Expected Frequencies: \\n{expected}\")\n"
      ],
      "metadata": {
        "id": "iqsnRVgJ5pcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Assuming your DataFrame is already loaded into variable df\n",
        "\n",
        "# Prepare your data for the Chi-Squared test\n",
        "# You might need to categorize or bin your 'E1' data if it's not already categorical\n",
        "# For this example, let's assume 'E1' is already categorical\n",
        "\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(df['Teaching_plan'], df['E1'])\n",
        "\n",
        "# Perform the Chi-Squared test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Output the results\n",
        "print('chi2=%.3f, p=%.3f, dof=%d' % (chi2, p, dof))\n",
        "\n",
        "# Interpret the result\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "    print('No significant association (fail to reject H0)')\n",
        "else:\n",
        "    print('Significant association (reject H0)')\n"
      ],
      "metadata": {
        "id": "sDBcEK0h1ioI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chi-Squared test: Teaching_plan = 4, 5, 6 vs. 1, 2, 3 for A1 responses (prior training)"
      ],
      "metadata": {
        "id": "cuUIZ-qfB9gX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Sample DataFrame setup\n",
        "# df = your dataframe\n",
        "\n",
        "# Create a contingency table\n",
        "# First, categorize the 'Teaching_plan' column into two groups\n",
        "df['Teaching_plan_group'] = df['Teaching_plan'].apply(lambda x: '4-6' if x in [4, 5, 6] else '1-3')\n",
        "\n",
        "# Create the contingency table for 'A1' across the 'Teaching_plan_group'\n",
        "contingency_table = pd.crosstab(df['A1'], df['Teaching_plan_group'])\n",
        "\n",
        "# Perform the Chi-Squared test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"Chi-Squared Test statistic:\", chi2)\n",
        "print(\"p-value:\", p)\n",
        "print(\"Degrees of freedom:\", dof)\n",
        "print(\"Expected frequencies table:\\n\", expected)\n"
      ],
      "metadata": {
        "id": "kgpi3Z_kOdBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.graphics.mosaicplot import mosaic\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9wlgyiIAC07_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Figure 1 Associate plot**"
      ],
      "metadata": {
        "id": "vrtfmJF7zuK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from statsmodels.graphics.mosaicplot import mosaic\n",
        "\n",
        "# ... your previous code for data preparation ...\n",
        "\n",
        "# Create a figure and axis object\n",
        "fig, ax = plt.subplots(figsize=(10, 6))  # Set the figure size\n",
        "\n",
        "# Create the mosaic plot using the ax object\n",
        "props = {}\n",
        "props[('4', 'YES')] = {'facecolor': 'blue', 'edgecolor': 'white'}\n",
        "props[('4', 'NO')] = {'facecolor': 'lightblue', 'edgecolor': 'white'}\n",
        "# ... set properties for other categories as needed ...\n",
        "\n",
        "mosaic(df, ['Teaching_plan', 'A1'], ax=ax, properties=props)\n",
        "\n",
        "# Set the x-label at the bottom and y-label using the axis object 'ax'\n",
        "ax.set_xlabel('Career Intent for English Teaching (N=228)', fontsize=14, labelpad=20)  # Set x-label with font size and padding\n",
        "ax.set_ylabel('Prior Exposure to Coding and Digital Literacy', fontsize=14)    # Set y-label with font size\n",
        "\n",
        "# Set title using 'ax' to ensure it's applied to the correct subplot\n",
        "ax.set_title('Association between Teacher Career and Prior Exposure to Digital Literacy Training', fontsize=16)\n",
        "\n",
        "# Save the figure with high resolution for publication\n",
        "plt.savefig('association_plot.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ELc3X6YO0Bdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2\n",
        "\n",
        "Figure 2 => Excel"
      ],
      "metadata": {
        "id": "AALnQKi5z0Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure 3 Heatmap (Teaching_plan) over importance of digital literacy"
      ],
      "metadata": {
        "id": "GtYWue_G0YBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C3, Teaching_plan"
      ],
      "metadata": {
        "id": "OY_1WaUE1Ogu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df is your DataFrame with 'C3' and 'Teaching_plan' columns\n",
        "\n",
        "# Create a Cross-Tabulation\n",
        "cross_tab = pd.crosstab(df['C3'], df['Teaching_plan'])\n",
        "\n",
        "# Normalize the counts by column to get ratios for each 'C3'\n",
        "normalized_cross_tab = cross_tab.div(cross_tab.sum(axis=0), axis=1)\n",
        "\n",
        "# Generate the Heatmap with Ratios\n",
        "plt.figure(figsize=(18, 6))\n",
        "heatmap = sns.heatmap(normalized_cross_tab, annot=True, cmap=\"YlGnBu\")\n",
        "plt.title('Heatmap of perceived importance (Ratios) of digital litearcy training by teaching Plan')\n",
        "\n",
        "# Set the x and y labels\n",
        "plt.ylabel('Perceived importance of digital literacy training')\n",
        "plt.xlabel('Teaching career plan')\n",
        "\n",
        "# Invert the y-axis\n",
        "heatmap.invert_yaxis()\n",
        "\n",
        "plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qvRYqFCx0zJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure 4: Excel\n",
        "Figure 5: heatmap of readiness"
      ],
      "metadata": {
        "id": "jNpAObJE1nVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df is your DataFrame with 'C3' and 'Teaching_plan' columns\n",
        "\n",
        "# Create a Cross-Tabulation\n",
        "cross_tab = pd.crosstab(df['C8'], df['Teaching_plan'])\n",
        "\n",
        "# Normalize the counts by column to get ratios for each 'C3'\n",
        "normalized_cross_tab = cross_tab.div(cross_tab.sum(axis=0), axis=1)\n",
        "\n",
        "# Generate the Heatmap with Ratios\n",
        "plt.figure(figsize=(18, 6))\n",
        "heatmap = sns.heatmap(normalized_cross_tab, annot=True, cmap=\"YlGnBu\")\n",
        "plt.title('Heatmap of Readiness (Ratios) of digital litearcy training by teaching Plan')\n",
        "\n",
        "# Set the x and y labels\n",
        "plt.ylabel('Readiness for digital literacy training')\n",
        "plt.xlabel('Teaching career plan')\n",
        "\n",
        "# Invert the y-axis\n",
        "heatmap.invert_yaxis()\n",
        "\n",
        "plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YkFlpTIt0hx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table 4 Descriptive statistics of the essay responses"
      ],
      "metadata": {
        "id": "64bBVCw5105f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your existing DataFrame\n",
        "\n",
        "# Function to count words in a string, considering 'NA' as 0 words\n",
        "def count_words(text):\n",
        "    # Check if text is 'NA', NaN, or empty, return 0 if true\n",
        "    if pd.isna(text) or text == 'NA':\n",
        "        return 0\n",
        "    # Split the text into words using space as a separator\n",
        "    words = text.split()\n",
        "    # Return the number of words\n",
        "    return len(words)\n",
        "\n",
        "# Apply the function to each row in the 'E3E' column\n",
        "# and store the result in a new 'NW' column\n",
        "df['NW'] = df['Essay'].apply(count_words)"
      ],
      "metadata": {
        "id": "PIZApc9e15vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/DLtotal_1116_wordcounts.csv\", encoding=\"UTF-8\")"
      ],
      "metadata": {
        "id": "wdRiJYDY20LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your existing DataFrame\n",
        "\n",
        "# Create a new DataFrame tdf with only the 'E3E' column from df\n",
        "tdf = df[['E3E', 'NW']].copy()\n",
        "\n",
        "# Display the new DataFrame to check the result\n",
        "print(tdf)\n",
        "\n",
        "tdf[-1:]"
      ],
      "metadata": {
        "id": "A5sciC60yoEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Table 4: Descriptive statistics: Individual responses"
      ],
      "metadata": {
        "id": "TF3y6wqm4OZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your existing DataFrame and 'NW' column is already created\n",
        "\n",
        "# Filter out rows where 'NW' is 0\n",
        "filtered_df = df[df['NW'] != 0]\n",
        "\n",
        "# Generate descriptive statistics for the 'NW' column\n",
        "descriptive_stats = filtered_df['NW'].describe()\n",
        "\n",
        "# Display the descriptive statistics\n",
        "print(descriptive_stats)\n"
      ],
      "metadata": {
        "id": "ex2mF7l41NOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your existing DataFrame and 'NW' column is already created\n",
        "\n",
        "# Calculate the sum of the 'NW' column\n",
        "nw_sum = df['NW'].sum()\n",
        "\n",
        "# Display the sum\n",
        "print(\"Sum of 'NW':\", nw_sum)\n"
      ],
      "metadata": {
        "id": "RnaJFV314NnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your existing DataFrame and 'NW' column is already created\n",
        "\n",
        "# Filter and display rows where 'NW' is equal to 1\n",
        "rows_with_nw_1 = df[df['NW'] == 4]\n",
        "\n",
        "# Display the rows\n",
        "print(rows_with_nw_1['Essay'])\n"
      ],
      "metadata": {
        "id": "wjaCU4Pp4jfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 6: Wordcloud"
      ],
      "metadata": {
        "id": "odWirgKr6N5k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BaPVLZDD6u8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your existing DataFrame and 'NW' column is already created\n",
        "\n",
        "# Filter and display rows where 'NW' is equal to 1\n",
        "rows_with_nw_1 = df[df['NW'] == 4]\n",
        "\n",
        "# Display the rows\n",
        "print(rows_with_nw_1['Essay'])\n"
      ],
      "metadata": {
        "id": "pfUeIihN63AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "sHXU93NjwbrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppn-NfO6uSti"
      },
      "outputs": [],
      "source": [
        "# Combine all rows in the 'reflection' column into one string\n",
        "# text = ' '.join(df['Reflection'])\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# You'll need to have a list of Korean stop words available\n",
        "stop_words = [\"n't\", 'think',\"thing\",\"I\",\"class\",\"classes\",\"taking\",\"course\",\"used\",\"using\"]  # replace this with your actual list of Korean stop words\n",
        "\n",
        "# Remove stop words\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Create a WordCloud\n",
        "wordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(tokens))\n",
        "\n",
        "# wordcloud = WordCloud(\n",
        "#     font_path='/content/your_font_file.ttf',  # replace with the name of your uploaded font file\n",
        "#     width = 1000,\n",
        "#     height = 500\n",
        "# ).generate(' '.join(tokens))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zZX24dVZ688T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your existing DataFrame and 'NW' column is already created\n",
        "\n",
        "# Filter and display rows where 'NW' is equal to 1\n",
        "rows_with_nw_1 = df[df['NW'] == 4]\n",
        "\n",
        "# Display the rows\n",
        "print(rows_with_nw_1['Essay'])\n"
      ],
      "metadata": {
        "id": "t7tVkh2w69Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "X6u3cp-J69Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all rows in the 'E3E' column into one string\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# You'll need to have a list of Korean stop words available\n",
        "stop_words = [\"'m\",\"n't\", 'think', \"thing\", \"I\", \"class\", \"classes\", \"taking\", \"course\", \"used\", \"using\"]  # replace this with your actual list of Korean stop words\n",
        "\n",
        "# Remove stop words\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Create a WordCloud with white background\n",
        "wordcloud = WordCloud(\n",
        "    width = 1000,\n",
        "    height = 500,\n",
        "    background_color='white'  # Set the background to white\n",
        ").generate(' '.join(tokens))\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"wordcloud.png\", format=\"png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "12ynSFyW9anZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import ImageColorGenerator\n",
        "import random\n",
        "\n",
        "# Combine all rows in the 'E3E' column into one string\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# List of Korean stop words\n",
        "stop_words = [\"n't\", 'think', \"thing\", \"I\", \"class\", \"classes\", \"taking\", \"course\", \"used\", \"using\"]  # replace with actual Korean stop words\n",
        "\n",
        "# Remove stop words\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Function to create a color scheme\n",
        "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
        "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
        "\n",
        "# Create a WordCloud with white background and darker text colors\n",
        "wordcloud = WordCloud(\n",
        "    width = 1000,\n",
        "    height = 500,\n",
        "    background_color='white',\n",
        "    color_func=grey_color_func  # Use the grey color function for text\n",
        ").generate(' '.join(tokens))\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nPILsSu4-u4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I0q3hmWR7GpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your existing DataFrame and 'NW' column is already created\n",
        "\n",
        "# Filter and display rows where 'NW' is equal to 1\n",
        "rows_with_nw_1 = df[df['NW'] == 4]\n",
        "\n",
        "# Display the rows\n",
        "print(rows_with_nw_1['Essay'])\n"
      ],
      "metadata": {
        "id": "OK4j0aRX7KQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "1Aillra-7KQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoLxft2r7KQw"
      },
      "outputs": [],
      "source": [
        "# Combine all rows in the 'reflection' column into one string\n",
        "# text = ' '.join(df['Reflection'])\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# You'll need to have a list of Korean stop words available\n",
        "stop_words = [\"n't\", 'think',\"thing\",\"I\",\"class\",\"classes\",\"taking\",\"course\",\"used\",\"using\"]  # replace this with your actual list of Korean stop words\n",
        "\n",
        "# Remove stop words\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Create a WordCloud\n",
        "wordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(tokens))\n",
        "\n",
        "# wordcloud = WordCloud(\n",
        "#     font_path='/content/your_font_file.ttf',  # replace with the name of your uploaded font file\n",
        "#     width = 1000,\n",
        "#     height = 500\n",
        "# ).generate(' '.join(tokens))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all rows in the 'E3E' column into one string\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# You'll need to have a list of Korean stop words available\n",
        "stop_words = [\"'m\",\"n't\", 'think', \"thing\", \"I\", \"class\", \"classes\", \"taking\", \"course\", \"used\", \"using\"]  # replace this with your actual list of Korean stop words\n",
        "\n",
        "# Remove stop words\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Create a WordCloud with white background\n",
        "wordcloud = WordCloud(\n",
        "    width = 1000,\n",
        "    height = 500,\n",
        "    background_color='white'  # Set the background to white\n",
        ").generate(' '.join(tokens))\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"wordcloud.png\", format=\"png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6v7MwIZe7KQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import ImageColorGenerator\n",
        "import random\n",
        "\n",
        "# Combine all rows in the 'E3E' column into one string\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# List of Korean stop words\n",
        "stop_words = [\"n't\", 'think', \"thing\", \"I\", \"class\", \"classes\", \"taking\", \"course\", \"used\", \"using\"]  # replace with actual Korean stop words\n",
        "\n",
        "# Remove stop words\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Function to create a color scheme\n",
        "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
        "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
        "\n",
        "# Create a WordCloud with white background and darker text colors\n",
        "wordcloud = WordCloud(\n",
        "    width = 1000,\n",
        "    height = 500,\n",
        "    background_color='white',\n",
        "    color_func=grey_color_func  # Use the grey color function for text\n",
        ").generate(' '.join(tokens))\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dcwTGqTH7KQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk wordcloud matplotlib"
      ],
      "metadata": {
        "id": "L4SEd0VkAJjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "QBvsbRW8AN0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "# Assuming df is your DataFrame and 'E3E' column contains the text\n",
        "\n",
        "# Function to extract adjectives from a string\n",
        "def extract_adjectives(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    adjectives = [word for word, tag in tagged if tag == 'JJ']\n",
        "    return ' '.join(adjectives)\n",
        "\n",
        "# Combine all rows in the 'E3E' column into one string and extract adjectives\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "adjectives_text = extract_adjectives(text)\n",
        "\n",
        "# Create a WordCloud\n",
        "wordcloud = WordCloud(\n",
        "    width = 1000,\n",
        "    height = 500,\n",
        "    background_color='white'\n",
        ").generate(adjectives_text)\n",
        "\n",
        "# Display the WordCloud\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"wordcloud_adj.png\", format=\"png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4XGT2IsEAQ46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noun wordcloud"
      ],
      "metadata": {
        "id": "DIt5v3yV7Z56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "# Assuming df is your DataFrame and 'E3E' column contains the text\n",
        "\n",
        "# Function to extract nouns from a string\n",
        "def extract_nouns(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    nouns = [word for word, tag in tagged if tag in ('NN', 'NNS', 'NNP', 'NNPS')]\n",
        "    return ' '.join(nouns)\n",
        "\n",
        "# Combine all rows in the 'E3E' column into one string and extract nouns\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "nouns_text = extract_nouns(text)\n",
        "\n",
        "# Create a WordCloud\n",
        "wordcloud = WordCloud(\n",
        "    width = 1000,\n",
        "    height = 500,\n",
        "    background_color='white'\n",
        ").generate(nouns_text)\n",
        "\n",
        "# Display the WordCloud\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"wordcloud_noun.png\", format=\"png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "-5qM6XlDBrO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Download the necessary tokenizer\n",
        "\n",
        "# Sample text\n",
        "text1 = adjectives_text\n",
        "text2 = nouns_text\n",
        "# Tokenize the text into words\n",
        "words1 = nltk.word_tokenize(text1)\n",
        "words2 = nltk.word_tokenize(text2)\n",
        "# Count the number of words\n",
        "word_count1 = len(words1)\n",
        "word_count2 = len(words2)\n",
        "\n",
        "print(f\"The number of adj words in the text is: {word_count1}\")\n",
        "print(f\"The number of noun words in the text is: {word_count2}\")"
      ],
      "metadata": {
        "id": "UUcfsdyaDt8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis"
      ],
      "metadata": {
        "id": "4btFpt0u7ctp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rUcjgh637fJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.text import Text\n",
        "\n",
        "# Let's say you want to extract the context of the word \"example\" in the 'reflection' column of your DataFrame\n",
        "word = 'never'\n",
        "\n",
        "# Combine all rows in the 'reflection' column into one string\n",
        "text = ' '.join(df['E3E'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Create a Text object\n",
        "text_obj = Text(tokens)\n",
        "\n",
        "# Create a concordance (KWIC) for the word\n",
        "text_obj.concordance(word)\n"
      ],
      "metadata": {
        "id": "u-eMmUKQ8f64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "C3-TuBg141Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment analysis\n",
        "# Please note that SentimentIntensityAnalyzer is designed for English text, so it may not give accurate results for Korean text\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "print(sentiment_scores)"
      ],
      "metadata": {
        "id": "X8nkUtGuxFUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ARK34BGY71do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result description**: The sentiment analysis of the text reveals a predominantly neutral tone, with 81.1% of the content classified as such. Positive sentiment constitutes 14.7% of the text, while negative sentiment is minimal, accounting for only 4.2%. Most notably, the compound score, a metric that encapsulates the overall sentiment of the text, is extremely high at 0.9999 (on a scale from -1 to 1). This suggests that despite the considerable proportion of neutral content, the text as a whole leans heavily towards a positive sentiment.\n",
        "\n",
        "+ 'neg': This is the Negative sentiment score. It represents the proportion of the text that is classified as negative. In your case, 4.2% of the text is classified as negative.\n",
        "\n",
        "+ 'neu': This is the Neutral sentiment score. It represents the proportion of the text that is classified as neutral. In your case, 81.1% of the text is classified as neutral.\n",
        "\n",
        "+ 'pos': This is the Positive sentiment score. It represents the proportion of the text that is classified as positive. In your case, 14.7% of the text is classified as positive.\n",
        "\n",
        "+ 'compound': This is the Compound score. It is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1 (most extreme negative) and +1 (most extreme positive). This score is a good indicator of the overall sentiment of the text. Positive values indicate positive sentiment, negative values indicate negative sentiment, and values close to zero indicate neutral sentiment. In your case, the compound score is 0.9999, which is very close to 1, indicating a very strong positive sentiment overall.\n",
        "+ The compound score in the NLTK's SentimentIntensityAnalyzer is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1 (most extreme negative) and +1 (most extreme positive).\n",
        "\n",
        "The calculation of the compound score is a bit complex. It uses the valence scores of each word in the text, which are determined by the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tool. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n",
        "\n",
        "Here is a simplified explanation of how it works:\n",
        "\n",
        "Each word in the text is assigned a valence score from the VADER lexicon. The valence score of a word is a measure of its emotional intensity ranging from -4 to +4. For example, the word \"love\" has a valence score of 3.2.\n",
        "\n",
        "These scores are adjusted based on the context of the word in the text. For example, if a word is in ALL CAPS, its valence score is increased. If a word is preceded by a negation word (like \"not\" or \"never\"), its valence score is reversed.\n",
        "\n",
        "The adjusted scores are then summed up and normalized to be between -1 and +1 to get the compound score.\n",
        "\n",
        "The exact calculation involves some additional steps and is beyond the scope of this explanation. If you're interested in the details, you can check out the source code of the SentimentIntensityAnalyzer or the original paper on VADER.\n",
        "\n",
        "Please note that the compound score is a good indicator of the overall sentiment of the text, but it may not always accurately reflect the sentiment of individual sentences or phrases within the text."
      ],
      "metadata": {
        "id": "QCudxDbF5x5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sentiment analysis\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(sentiment_scores.keys(), sentiment_scores.values())\n",
        "plt.title('Sentiment Scores')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WybKmBPZ5How"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Assuming 'text' is predefined and contains the text to be analyzed\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "# Exclude the 'compound' score for the pie chart\n",
        "scores_for_pie = {key: value for key, value in sentiment_scores.items() if key != 'compound'}\n",
        "\n",
        "# Create a pie chart\n",
        "plt.pie(scores_for_pie.values(), labels=scores_for_pie.keys(), autopct='%1.1f%%')\n",
        "plt.title('Sentiment Scores')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "kfAvuAbV6CxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Assuming 'text' is predefined and contains the text to be analyzed\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "# Specify colors for each sentiment category\n",
        "colors = ['red', 'gray', 'blue', 'orange']\n",
        "\n",
        "# Adjust the figure size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create a horizontal bar plot with specified colors\n",
        "plt.barh(list(sentiment_scores.keys()), list(sentiment_scores.values()), color=colors)\n",
        "plt.title('Sentiment Scores')\n",
        "plt.savefig(\"sentiment_updated.png\", format=\"png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "16zhBb986JnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Figure 7 Sentiment scores bar plot"
      ],
      "metadata": {
        "id": "lQTgd-m98DH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Assuming 'text' is predefined and contains the text to be analyzed\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "# Specify colors for each sentiment category\n",
        "colors = ['red', 'gray', 'blue', 'orange']\n",
        "\n",
        "# Adjust the figure size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create a horizontal bar plot with specified colors\n",
        "bars = plt.barh(list(sentiment_scores.keys()), list(sentiment_scores.values()), color=colors)\n",
        "\n",
        "# Text labels with the scores\n",
        "scores = [0.135,0.809, 0.056, -0.9998]\n",
        "\n",
        "# Iterate over bars and scores to place text labels\n",
        "for bar, score in zip(bars, scores):\n",
        "    # Position the text inside or right side of each bar\n",
        "    plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2,\n",
        "             f'{score}', va='center')\n",
        "\n",
        "plt.title('Sentiment Scores')\n",
        "plt.savefig(\"sentiment_updated.png\", format=\"png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SkvtdiY72vqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network analysis"
      ],
      "metadata": {
        "id": "SuZAEdHZ8Ime"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvis"
      ],
      "metadata": {
        "id": "sv3tyfvBXZrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown This codes saves html of network but doesn't display on colab\n",
        "import nltk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "from pyvis.network import Network\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text into sentences and then into words\n",
        "sentences = sent_tokenize(text)\n",
        "tokens = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Remove punctuations and specific words\n",
        "tokens = [[word for word in sent if word not in string.punctuation and word not in [\"n't\", \"think\", \"thought\",\"used\",\"using\",\"class\",\"classes\",\"course\",\"taking\",\"'s\",\"things\"]] for sent in tokens]\n",
        "\n",
        "# Flatten the list of sentences into a list of words\n",
        "flat_tokens = [word for sent in tokens for word in sent]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(flat_tokens)\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 10  # Adjust the threshold as needed\n",
        "\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes for each filtered token\n",
        "G.add_nodes_from(filtered_tokens)\n",
        "\n",
        "# Add edges based on co-occurrence within the same sentence\n",
        "for sent in tokens:\n",
        "    for i in range(len(sent) - 1):\n",
        "        if sent[i] in filtered_tokens and sent[i+1] in filtered_tokens:\n",
        "            G.add_edge(sent[i], sent[i+1])\n",
        "\n",
        "# Create a Network instance\n",
        "nt = Network(notebook=True, height=\"500px\", width=\"100%\", cdn_resources='remote')\n",
        "\n",
        "# Add nodes and edges to the network\n",
        "for node in G.nodes:\n",
        "    nt.add_node(node)\n",
        "for edge in G.edges:\n",
        "    nt.add_edge(edge[0], edge[1])\n",
        "\n",
        "# Visualize the network graph\n",
        "nt.show(\"network_graph.html\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TG8Hni-LXTcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust to Ipython for display"
      ],
      "metadata": {
        "id": "KqTBxZx6ZqKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Network graph (works well)\n",
        "import nltk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "from pyvis.network import Network\n",
        "from IPython.display import HTML\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text into sentences and then into words\n",
        "sentences = sent_tokenize(text)\n",
        "tokens = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Convert tokens to lowercase\n",
        "tokens = [[word.lower() for word in sent if word not in string.punctuation and word not in [\"n't\", \"think\", \"thought\", \"used\", \"using\", \"class\", \"classes\", \"course\", \"taking\", \"'s\", \"things\"]] for sent in tokens]\n",
        "\n",
        "#\n",
        "\n",
        "# Remove punctuations and specific words\n",
        "# tokens = [[word for word in sent if word not in string.punctuation and word not in [\"n't\", \"think\", \"thought\",\"used\",\"using\",\"class\",\"classes\",\"course\",\"taking\",\"'s\",\"things\"]] for sent in tokens]\n",
        "tokens = [[word for word in sent if word not in string.punctuation] for sent in tokens]\n",
        "# Flatten the list of sentences into a list of words\n",
        "flat_tokens = [word for sent in tokens for word in sent]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(flat_tokens)\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = input('Type threshold: e.g., 5, 10')  # Adjust the threshold as needed\n",
        "threshold = int(threshold)\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes for each filtered token\n",
        "G.add_nodes_from(filtered_tokens)\n",
        "\n",
        "# Add edges based on co-occurrence within the same sentence\n",
        "for sent in tokens:\n",
        "    for i in range(len(sent) - 1):\n",
        "        if sent[i] in filtered_tokens and sent[i+1] in filtered_tokens:\n",
        "            G.add_edge(sent[i], sent[i+1])\n",
        "\n",
        "# nt.set_options(options)\n",
        "\n",
        "# Add nodes and edges to the network\n",
        "for node in G.nodes:\n",
        "    nt.add_node(node)\n",
        "for edge in G.edges:\n",
        "    nt.add_edge(edge[0], edge[1])\n",
        "\n",
        "# Visualize the network graph and save it to an HTML file\n",
        "nt.save_graph(\"network_graph.html\")\n",
        "\n",
        "# Display the HTML file in Colab\n",
        "display(HTML(\"network_graph.html\"))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mUmsiZYtZseB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting optimal threshold: using frequency density calculation"
      ],
      "metadata": {
        "id": "5-Ozd_7FsamV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Density calculation and graph:\n",
        "import nltk\n",
        "import networkx as nx\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text into sentences and then into words\n",
        "sentences = sent_tokenize(text)\n",
        "tokens = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Initialize WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize tokens\n",
        "lemmatized_tokens = [[lemmatizer.lemmatize(word) for word in sent] for sent in tokens]\n",
        "\n",
        "# Convert tokens to lowercase and remove punctuation\n",
        "lemmatized_tokens = [[word.lower() for word in sent if word not in string.punctuation] for sent in lemmatized_tokens]\n",
        "\n",
        "# Flatten the list of sentences into a list of words\n",
        "flat_lemmatized_tokens = [word for sent in lemmatized_tokens for word in sent]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(flat_lemmatized_tokens)\n",
        "\n",
        "# Calculate the total number of possible edges (in a complete graph)\n",
        "total_possible_edges = len(set(flat_lemmatized_tokens)) * (len(set(flat_lemmatized_tokens)) - 1)\n",
        "\n",
        "# Define a function to calculate the density of the graph\n",
        "def calculate_density(threshold, word_freq):\n",
        "    high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "    G = nx.DiGraph()\n",
        "    G.add_nodes_from(high_freq_words)\n",
        "    for sent in lemmatized_tokens:\n",
        "        for i in range(len(sent) - 1):\n",
        "            if sent[i] in high_freq_words and sent[i+1] in high_freq_words:\n",
        "                G.add_edge(sent[i], sent[i+1])\n",
        "    current_density = nx.density(G)\n",
        "    return current_density\n",
        "\n",
        "# Define your target density here\n",
        "target_density = 0.1  # for example, 10%\n",
        "\n",
        "# Initialize threshold to a value that will be updated\n",
        "threshold = 2  # starting with a low threshold\n",
        "current_density = calculate_density(threshold, word_freq)\n",
        "\n",
        "# Adjust threshold until the density is close to the target density\n",
        "while current_density < target_density and threshold < max(word_freq.values()):\n",
        "    threshold += 1\n",
        "    current_density = calculate_density(threshold, word_freq)\n",
        "\n",
        "print(f\"Optimal threshold: {threshold}\")\n",
        "print(f\"Resulting density: {current_density}\")\n",
        "\n",
        "# Calculate densities for a range of thresholds\n",
        "thresholds = range(2, max(word_freq.values()) + 1)\n",
        "densities = [calculate_density(threshold, word_freq) for threshold in thresholds]\n",
        "\n",
        "# Find the threshold closest to the target density\n",
        "# and its corresponding index\n",
        "closest_threshold = min(thresholds, key=lambda t: abs(calculate_density(t, word_freq) - target_density))\n",
        "index_of_closest_threshold = thresholds.index(closest_threshold)\n",
        "\n",
        "# Plot the density graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(thresholds, densities, marker='o')\n",
        "plt.axhline(y=target_density, color='r', linestyle='--', label=f'Target Density ({target_density})')\n",
        "plt.axvline(x=closest_threshold, color='g', linestyle='--', label=f'Closest Threshold ({closest_threshold})')\n",
        "plt.title('Graph Density vs. Word Frequency Threshold')\n",
        "plt.xlabel('Word Frequency Threshold')\n",
        "plt.ylabel('Graph Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('density_graph.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TZ0iP_mEokf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# network analysis with lemmatized tokens (final version?)"
      ],
      "metadata": {
        "id": "RPdUfJxPfe1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Network graph (final version?): threshold set to 11\n",
        "import nltk\n",
        "import networkx as nx\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import string\n",
        "from pyvis.network import Network\n",
        "from IPython.display import HTML\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text into sentences and then into words\n",
        "sentences = sent_tokenize(text)\n",
        "tokens = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Initialize WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize tokens\n",
        "lemmatized_tokens = [[lemmatizer.lemmatize(word) for word in sent] for sent in tokens]\n",
        "\n",
        "# Convert tokens to lowercase and remove punctuation\n",
        "lemmatized_tokens = [[word.lower() for word in sent if word not in string.punctuation] for sent in lemmatized_tokens]\n",
        "\n",
        "# Flatten the list of sentences into a list of words\n",
        "flat_lemmatized_tokens = [word for sent in lemmatized_tokens for word in sent]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(flat_lemmatized_tokens)\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 11  # Adjust the threshold as needed\n",
        "\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes for each filtered token\n",
        "G.add_nodes_from(filtered_tokens)\n",
        "\n",
        "# Add edges based on co-occurrence within the same sentence\n",
        "for sent in lemmatized_tokens:\n",
        "    for i in range(len(sent) - 1):\n",
        "        if sent[i] in filtered_tokens and sent[i+1] in filtered_tokens:\n",
        "            G.add_edge(sent[i], sent[i+1])\n",
        "\n",
        "# Create a Network instance\n",
        "nt = Network(notebook=True, height=\"800px\", width=\"100%\", cdn_resources='remote')\n",
        "\n",
        "# Add nodes and edges to the network\n",
        "for node in G.nodes:\n",
        "    nt.add_node(node)\n",
        "for edge in G.edges:\n",
        "    nt.add_edge(edge[0], edge[1])\n",
        "\n",
        "# Visualize the network graph and save it to an HTML file\n",
        "nt.save_graph(\"network_graph0.html\")\n",
        "\n",
        "# Display the HTML file in Colab\n",
        "display(HTML(\"network_graph0.html\"))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Zti2fXBEfedm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_nodes = G.number_of_nodes()\n",
        "print(number_of_nodes)\n"
      ],
      "metadata": {
        "id": "zeC7-09XmcQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the threshold using density calculation"
      ],
      "metadata": {
        "id": "DjnZNF6aohSq"
      }
    }
  ]
}