{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNKMJ85SdqsH2L5WM6MLdSS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/workspace/blob/main/DLP/DL_perception_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DLP Step2: Analyzing the essay responses using NLP\n",
        "\n",
        "215 responses out of 228"
      ],
      "metadata": {
        "id": "qsfXUFMGxFEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1] File to read: variables df and info"
      ],
      "metadata": {
        "id": "aABNju2uxwmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHh_bz9-jSWY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/DLtotal_1113.csv\")\n",
        "df[-1:]\n",
        "\n",
        "info = pd.read_csv(\"/content/DLtotal_1113_heads.csv\")\n",
        "info[-1:]"
      ],
      "metadata": {
        "id": "jnnRAFxGkGhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2XfRdOJxDJr"
      },
      "outputs": [],
      "source": [
        "# Get the column names as a list\n",
        "column_names = list(df.columns)\n",
        "\n",
        "print(column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [2] Select essay texts and process"
      ],
      "metadata": {
        "id": "dgnfTaEvx0od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a new column names 'NW' number of words"
      ],
      "metadata": {
        "id": "nsxrUhbp2uZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your existing DataFrame\n",
        "\n",
        "# Function to count words in a string, considering 'NA' as 0 words\n",
        "def count_words(text):\n",
        "    # Check if text is 'NA', NaN, or empty, return 0 if true\n",
        "    if pd.isna(text) or text == 'NA':\n",
        "        return 0\n",
        "    # Split the text into words using space as a separator\n",
        "    words = text.split()\n",
        "    # Return the number of words\n",
        "    return len(words)\n",
        "\n",
        "# Apply the function to each row in the 'E3E' column\n",
        "# and store the result in a new 'NW' column\n",
        "df['NW'] = df['Essay'].apply(count_words)"
      ],
      "metadata": {
        "id": "SA7MmW4o2agO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/DLtotal_1113_wordcounts.csv\", encoding=\"UTF-8\")"
      ],
      "metadata": {
        "id": "wdRiJYDY20LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your existing DataFrame\n",
        "\n",
        "# Create a new DataFrame tdf with only the 'E3E' column from df\n",
        "tdf = df[['E3E', 'NW']].copy()\n",
        "\n",
        "# Display the new DataFrame to check the result\n",
        "print(tdf)\n",
        "\n",
        "tdf[-1:]"
      ],
      "metadata": {
        "id": "A5sciC60yoEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descriptive statistics: Individual responses"
      ],
      "metadata": {
        "id": "TF3y6wqm4OZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your existing DataFrame and 'NW' column is already created\n",
        "\n",
        "# Filter out rows where 'NW' is 0\n",
        "filtered_df = df[df['NW'] != 0]\n",
        "\n",
        "# Generate descriptive statistics for the 'NW' column\n",
        "descriptive_stats = filtered_df['NW'].describe()\n",
        "\n",
        "# Display the descriptive statistics\n",
        "print(descriptive_stats)\n"
      ],
      "metadata": {
        "id": "ex2mF7l41NOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your existing DataFrame and 'NW' column is already created\n",
        "\n",
        "# Calculate the sum of the 'NW' column\n",
        "nw_sum = df['NW'].sum()\n",
        "\n",
        "# Display the sum\n",
        "print(\"Sum of 'NW':\", nw_sum)\n"
      ],
      "metadata": {
        "id": "RnaJFV314NnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your existing DataFrame and 'NW' column is already created\n",
        "\n",
        "# Filter and display rows where 'NW' is equal to 1\n",
        "rows_with_nw_1 = df[df['NW'] == 4]\n",
        "\n",
        "# Display the rows\n",
        "print(rows_with_nw_1['Essay'])\n"
      ],
      "metadata": {
        "id": "wjaCU4Pp4jfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis"
      ],
      "metadata": {
        "id": "HpJFszer8O4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "sHXU93NjwbrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file from GitHub\n",
        "# df = pd.read_csv('https://raw.githubusercontent.com/MK316/workspace/main/ifl23/data/reflection.csv', encoding='utf-8')"
      ],
      "metadata": {
        "id": "cHvIZMHG4fUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fSB4KM3p2nSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppn-NfO6uSti"
      },
      "outputs": [],
      "source": [
        "# Combine all rows in the 'reflection' column into one string\n",
        "# text = ' '.join(df['Reflection'])\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# You'll need to have a list of Korean stop words available\n",
        "stop_words = [\"n't\", 'think',\"thing\",\"I\",\"class\",\"classes\",\"taking\",\"course\",\"used\",\"using\"]  # replace this with your actual list of Korean stop words\n",
        "\n",
        "# Remove stop words\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Create a WordCloud\n",
        "wordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(tokens))\n",
        "\n",
        "# wordcloud = WordCloud(\n",
        "#     font_path='/content/your_font_file.ttf',  # replace with the name of your uploaded font file\n",
        "#     width = 1000,\n",
        "#     height = 500\n",
        "# ).generate(' '.join(tokens))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all rows in the 'E3E' column into one string\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# You'll need to have a list of Korean stop words available\n",
        "stop_words = [\"'m\",\"n't\", 'think', \"thing\", \"I\", \"class\", \"classes\", \"taking\", \"course\", \"used\", \"using\"]  # replace this with your actual list of Korean stop words\n",
        "\n",
        "# Remove stop words\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Create a WordCloud with white background\n",
        "wordcloud = WordCloud(\n",
        "    width = 1000,\n",
        "    height = 500,\n",
        "    background_color='white'  # Set the background to white\n",
        ").generate(' '.join(tokens))\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"wordcloud.png\", format=\"png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "12ynSFyW9anZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import ImageColorGenerator\n",
        "import random\n",
        "\n",
        "# Combine all rows in the 'E3E' column into one string\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# List of Korean stop words\n",
        "stop_words = [\"n't\", 'think', \"thing\", \"I\", \"class\", \"classes\", \"taking\", \"course\", \"used\", \"using\"]  # replace with actual Korean stop words\n",
        "\n",
        "# Remove stop words\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Function to create a color scheme\n",
        "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
        "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
        "\n",
        "# Create a WordCloud with white background and darker text colors\n",
        "wordcloud = WordCloud(\n",
        "    width = 1000,\n",
        "    height = 500,\n",
        "    background_color='white',\n",
        "    color_func=grey_color_func  # Use the grey color function for text\n",
        ").generate(' '.join(tokens))\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nPILsSu4-u4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract adjectives / nouns and draw wordcloud"
      ],
      "metadata": {
        "id": "FmJA6iotAFyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk wordcloud matplotlib"
      ],
      "metadata": {
        "id": "L4SEd0VkAJjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "QBvsbRW8AN0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "# Assuming df is your DataFrame and 'E3E' column contains the text\n",
        "\n",
        "# Function to extract adjectives from a string\n",
        "def extract_adjectives(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    adjectives = [word for word, tag in tagged if tag == 'JJ']\n",
        "    return ' '.join(adjectives)\n",
        "\n",
        "# Combine all rows in the 'E3E' column into one string and extract adjectives\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "adjectives_text = extract_adjectives(text)\n",
        "\n",
        "# Create a WordCloud\n",
        "wordcloud = WordCloud(\n",
        "    width = 1000,\n",
        "    height = 500,\n",
        "    background_color='white'\n",
        ").generate(adjectives_text)\n",
        "\n",
        "# Display the WordCloud\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"wordcloud_adj.png\", format=\"png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4XGT2IsEAQ46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "# Assuming df is your DataFrame and 'E3E' column contains the text\n",
        "\n",
        "# Function to extract nouns from a string\n",
        "def extract_nouns(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    nouns = [word for word, tag in tagged if tag in ('NN', 'NNS', 'NNP', 'NNPS')]\n",
        "    return ' '.join(nouns)\n",
        "\n",
        "# Combine all rows in the 'E3E' column into one string and extract nouns\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "nouns_text = extract_nouns(text)\n",
        "\n",
        "# Create a WordCloud\n",
        "wordcloud = WordCloud(\n",
        "    width = 1000,\n",
        "    height = 500,\n",
        "    background_color='white'\n",
        ").generate(nouns_text)\n",
        "\n",
        "# Display the WordCloud\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"wordcloud_noun.png\", format=\"png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "-5qM6XlDBrO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Download the necessary tokenizer\n",
        "\n",
        "# Sample text\n",
        "text1 = adjectives_text\n",
        "text2 = nouns_text\n",
        "# Tokenize the text into words\n",
        "words1 = nltk.word_tokenize(text1)\n",
        "words2 = nltk.word_tokenize(text2)\n",
        "# Count the number of words\n",
        "word_count1 = len(words1)\n",
        "word_count2 = len(words2)\n",
        "\n",
        "print(f\"The number of adj words in the text is: {word_count1}\")\n",
        "print(f\"The number of noun words in the text is: {word_count2}\")"
      ],
      "metadata": {
        "id": "UUcfsdyaDt8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis"
      ],
      "metadata": {
        "id": "xIOErzV8KFQe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lqZQwTpRKHoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.text import Text\n",
        "\n",
        "# Let's say you want to extract the context of the word \"example\" in the 'reflection' column of your DataFrame\n",
        "word = 'never'\n",
        "\n",
        "# Combine all rows in the 'reflection' column into one string\n",
        "text = ' '.join(df['E3E'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Create a Text object\n",
        "text_obj = Text(tokens)\n",
        "\n",
        "# Create a concordance (KWIC) for the word\n",
        "text_obj.concordance(word)\n"
      ],
      "metadata": {
        "id": "u-eMmUKQ8f64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "C3-TuBg141Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment analysis\n",
        "# Please note that SentimentIntensityAnalyzer is designed for English text, so it may not give accurate results for Korean text\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "print(sentiment_scores)"
      ],
      "metadata": {
        "id": "X8nkUtGuxFUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result description**: The sentiment analysis of the text reveals a predominantly neutral tone, with 81.1% of the content classified as such. Positive sentiment constitutes 14.7% of the text, while negative sentiment is minimal, accounting for only 4.2%. Most notably, the compound score, a metric that encapsulates the overall sentiment of the text, is extremely high at 0.9999 (on a scale from -1 to 1). This suggests that despite the considerable proportion of neutral content, the text as a whole leans heavily towards a positive sentiment.\n",
        "\n",
        "+ 'neg': This is the Negative sentiment score. It represents the proportion of the text that is classified as negative. In your case, 4.2% of the text is classified as negative.\n",
        "\n",
        "+ 'neu': This is the Neutral sentiment score. It represents the proportion of the text that is classified as neutral. In your case, 81.1% of the text is classified as neutral.\n",
        "\n",
        "+ 'pos': This is the Positive sentiment score. It represents the proportion of the text that is classified as positive. In your case, 14.7% of the text is classified as positive.\n",
        "\n",
        "+ 'compound': This is the Compound score. It is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1 (most extreme negative) and +1 (most extreme positive). This score is a good indicator of the overall sentiment of the text. Positive values indicate positive sentiment, negative values indicate negative sentiment, and values close to zero indicate neutral sentiment. In your case, the compound score is 0.9999, which is very close to 1, indicating a very strong positive sentiment overall.\n",
        "+ The compound score in the NLTK's SentimentIntensityAnalyzer is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1 (most extreme negative) and +1 (most extreme positive).\n",
        "\n",
        "The calculation of the compound score is a bit complex. It uses the valence scores of each word in the text, which are determined by the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tool. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n",
        "\n",
        "Here is a simplified explanation of how it works:\n",
        "\n",
        "Each word in the text is assigned a valence score from the VADER lexicon. The valence score of a word is a measure of its emotional intensity ranging from -4 to +4. For example, the word \"love\" has a valence score of 3.2.\n",
        "\n",
        "These scores are adjusted based on the context of the word in the text. For example, if a word is in ALL CAPS, its valence score is increased. If a word is preceded by a negation word (like \"not\" or \"never\"), its valence score is reversed.\n",
        "\n",
        "The adjusted scores are then summed up and normalized to be between -1 and +1 to get the compound score.\n",
        "\n",
        "The exact calculation involves some additional steps and is beyond the scope of this explanation. If you're interested in the details, you can check out the source code of the SentimentIntensityAnalyzer or the original paper on VADER.\n",
        "\n",
        "Please note that the compound score is a good indicator of the overall sentiment of the text, but it may not always accurately reflect the sentiment of individual sentences or phrases within the text."
      ],
      "metadata": {
        "id": "QCudxDbF5x5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sentiment analysis\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(sentiment_scores.keys(), sentiment_scores.values())\n",
        "plt.title('Sentiment Scores')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WybKmBPZ5How"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Assuming 'text' is predefined and contains the text to be analyzed\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "# Colors for each sentiment category\n",
        "colors = ['red', 'gray', 'blue', 'orange']\n",
        "\n",
        "# Create a bar plot with specified colors\n",
        "plt.bar(sentiment_scores.keys(), sentiment_scores.values(), color=colors)\n",
        "plt.title('Sentiment Scores')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6VtIVIxp3RQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Assuming 'text' is predefined and contains the text to be analyzed\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "# Exclude the 'compound' score for the pie chart\n",
        "scores_for_pie = {key: value for key, value in sentiment_scores.items() if key != 'compound'}\n",
        "\n",
        "# Create a pie chart\n",
        "plt.pie(scores_for_pie.values(), labels=scores_for_pie.keys(), autopct='%1.1f%%')\n",
        "plt.title('Sentiment Scores')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "kfAvuAbV6CxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Assuming 'text' is predefined and contains the text to be analyzed\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "# Specify colors for each sentiment category\n",
        "colors = ['red', 'gray', 'blue', 'orange']\n",
        "\n",
        "# Adjust the figure size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create a horizontal bar plot with specified colors\n",
        "plt.barh(list(sentiment_scores.keys()), list(sentiment_scores.values()), color=colors)\n",
        "plt.title('Sentiment Scores')\n",
        "plt.savefig(\"sentiment_updated.png\", format=\"png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "16zhBb986JnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Assuming 'text' is predefined and contains the text to be analyzed\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "# Specify colors for each sentiment category\n",
        "colors = ['red', 'gray', 'blue', 'orange']\n",
        "\n",
        "# Adjust the figure size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create a horizontal bar plot with specified colors\n",
        "bars = plt.barh(list(sentiment_scores.keys()), list(sentiment_scores.values()), color=colors)\n",
        "\n",
        "# Text labels with the scores\n",
        "scores = [0.135,0.809, 0.056, -0.9998]\n",
        "\n",
        "# Iterate over bars and scores to place text labels\n",
        "for bar, score in zip(bars, scores):\n",
        "    # Position the text inside or right side of each bar\n",
        "    plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2,\n",
        "             f'{score}', va='center')\n",
        "\n",
        "plt.title('Sentiment Scores')\n",
        "plt.savefig(\"sentiment_updated.png\", format=\"png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SkvtdiY72vqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network analysis"
      ],
      "metadata": {
        "id": "JpT6JV4sLT7w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uhM7C8gCLWHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Preprocessing and tokenization\n",
        "tokens = text.split()  # Split text into tokens\n",
        "\n",
        "# Build the network\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(tokens)  # Add nodes for each token\n",
        "\n",
        "# Add edges based on relationships (e.g., co-occurrence)\n",
        "for i in range(len(tokens) - 1):\n",
        "    G.add_edge(tokens[i], tokens[i+1])\n",
        "\n",
        "# Visualize the network graph\n",
        "pos = nx.spring_layout(G)  # Position nodes using a spring layout algorithm\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
        "plt.title(\"Network Graph\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l__8c-AKByd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Preprocessing and tokenization\n",
        "tokens = text.split()  # Split text into tokens\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 30  # Adjust the threshold as needed\n",
        "\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Build the network graph with high-frequency words\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(high_freq_words)  # Add nodes for each high-frequency word\n",
        "\n",
        "# Add edges based on relationships (e.g., co-occurrence)\n",
        "for i in range(len(tokens) - 1):\n",
        "    if tokens[i] in high_freq_words and tokens[i+1] in high_freq_words:\n",
        "        G.add_edge(tokens[i], tokens[i+1])\n",
        "\n",
        "# Visualize the network graph\n",
        "pos = nx.spring_layout(G)  # Position nodes using a spring layout algorithm\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
        "plt.title(\"Network Graph (High-Frequency Words)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Q1VrIpc0CG3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Preprocessing and tokenization\n",
        "tokens = text.split()  # Split text into tokens\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 30  # Adjust the threshold as needed\n",
        "\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Build the network graph with high-frequency words\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(high_freq_words)  # Add nodes for each high-frequency word\n",
        "\n",
        "# Add edges based on relationships (e.g., co-occurrence)\n",
        "for i in range(len(tokens) - 1):\n",
        "    if tokens[i] in high_freq_words and tokens[i+1] in high_freq_words:\n",
        "        G.add_edge(tokens[i], tokens[i+1])\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(12, 6))  # Set the figure size to 18x6 inches\n",
        "\n",
        "# Visualize the network graph\n",
        "pos = nx.spring_layout(G)  # Position nodes using a spring layout algorithm\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
        "plt.title(\"Network Graph (High-Frequency Words)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "X7vFBDg4Nh87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 20  # Adjust the threshold as needed\n",
        "\n",
        "# Get the high-frequency words\n",
        "# high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(filtered_tokens)  # Add nodes for each filtered token\n",
        "\n",
        "# Add edges based on relationships (e.g., co-occurrence)\n",
        "for i in range(len(filtered_tokens) - 1):\n",
        "    G.add_edge(filtered_tokens[i], filtered_tokens[i+1])\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Visualize the network graph\n",
        "pos = nx.spring_layout(G)  # Position nodes using a spring layout algorithm\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
        "plt.title(\"Network Graph (Without Stopwords)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l5VhY4zzCkpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove punctuation"
      ],
      "metadata": {
        "id": "dPidBPvZOeNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text and remove punctuation\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = input('Type number for threshold: e.g., 5, 10')  # Adjust the threshold as needed\n",
        "threshold = int(threshold)\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(filtered_tokens)  # Add nodes for each filtered token\n",
        "\n",
        "# Add edges based on relationships (e.g., co-occurrence)\n",
        "for i in range(len(filtered_tokens) - 1):\n",
        "    G.add_edge(filtered_tokens[i], filtered_tokens[i+1])\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Visualize the network graph\n",
        "pos = nx.spring_layout(G)  # Position nodes using a spring layout algorithm\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
        "plt.title(\"Network Graph (Without Stopwords)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GoAlLyuSOdsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text\n",
        "# Tokenize the text and remove punctuation\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "# Remove punctuations and \"n't\"\n",
        "# tokens = [word for word in tokens if word not in string.punctuation and word not in [\"n't\", \"think\", \"thought\",\"used\",\"using\",\"class\",\"classes\",\"course\",\"taking\",\"'s\",\"things\"]]\n",
        "\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 5  # Adjust the threshold as needed\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(filtered_tokens)  # Add nodes for each filtered token\n",
        "\n",
        "# Add edges based on relationships (e.g., co-occurrence)\n",
        "for i in range(len(filtered_tokens) - 1):\n",
        "    G.add_edge(filtered_tokens[i], filtered_tokens[i+1])\n",
        "\n",
        "plt.figure(figsize=(18, 9))  # Adjust the width and height as desired\n",
        "\n",
        "\n",
        "# Visualize the network graph\n",
        "pos = nx.spring_layout(G)  # Position nodes using a spring layout algorithm\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
        "plt.title(\"Network Graph (Without Stopwords)\")\n",
        "plt.savefig('network_graph.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L4sCI5YyEnCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "variation: n't = not, 's to remove"
      ],
      "metadata": {
        "id": "XGTV14GEOCcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Preprocess text: Replace \"n't\" with \"not\" and remove \"'s\"\n",
        "# text = text.replace(\"n't\", \" not\").replace(\"'s\", \"\").replace(\"'m\", \" am\").replace(\"ca\", \"\").replace(\"n\",\"\")\n",
        "\n",
        "# Tokenize the text and remove punctuation\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tokens = [word for word in tokens if word not in string.punctuation]\n",
        "# Remove punctuations\n",
        "tokens = [word for word in tokens if word not in string.punctuation and word not in [\"think\", \"thought\", \"used\", \"using\", \"class\", \"classes\", \"course\", \"taking\", \"things\"]]\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 15  # Adjust the threshold as needed\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(filtered_tokens)  # Add nodes for each filtered token\n",
        "\n",
        "# Add edges based on relationships (e.g., co-occurrence)\n",
        "for i in range(len(filtered_tokens) - 1):\n",
        "    G.add_edge(filtered_tokens[i], filtered_tokens[i+1])\n",
        "\n",
        "plt.figure(figsize=(18, 9))  # Adjust the width and height as desired\n",
        "\n",
        "# Visualize the network graph\n",
        "pos = nx.spring_layout(G)  # Position nodes using a spring layout algorithm\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
        "plt.title(\"Network Graph (Without Stopwords)\")\n",
        "plt.savefig('network_graph.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5lNE1o53OBWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Character trim issue"
      ],
      "metadata": {
        "id": "qQysKJhgRJso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Preprocess text: Replace \"n't\" with \"not\" and remove \"'s\"\n",
        "text = text.replace(\"n't\", \" not\").replace(\"'s\", \"\").replace(\"'m\", \" am\").replace(\"ca\", \"\").replace(\"n\",\"\")\n",
        "\n",
        "# Tokenize the text and remove punctuation\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tokens = [word for word in tokens if word not in string.punctuation]\n",
        "# Remove punctuations\n",
        "tokens = [word for word in tokens if word not in string.punctuation and word not in [\"think\", \"thought\", \"used\", \"using\", \"class\", \"classes\", \"course\", \"taking\", \"things\"]]\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 15  # Adjust the threshold as needed\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(filtered_tokens)  # Add nodes for each filtered token\n",
        "\n",
        "# Add edges based on relationships (e.g., co-occurrence)\n",
        "for i in range(len(filtered_tokens) - 1):\n",
        "    G.add_edge(filtered_tokens[i], filtered_tokens[i+1])\n",
        "\n",
        "# plt.figure(figsize=(18, 9))  # Adjust the width and height as desired\n",
        "# Set the figure size larger\n",
        "plt.figure(figsize=(18, 10))\n",
        "\n",
        "# Visualize the network graph\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, node_color='lightblue', edge_color='gray', font_size=6)\n",
        "plt.title(\"Network Graph (Without Stopwords)\")\n",
        "plt.savefig('network_graph.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dNj5143-RKB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After tokenization\n",
        "print(\"Tokens:\", tokens[:50])  # Print the first 50 tokens for inspection\n",
        "\n",
        "# After removing punctuation\n",
        "tokens = [word for word in tokens if word not in string.punctuation]\n",
        "print(\"Tokens after removing punctuation:\", tokens[:50])\n",
        "\n",
        "# After filtering high-frequency words and stopwords\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words and word in high_freq_words]\n",
        "print(\"Filtered Tokens:\", filtered_tokens[:50])\n"
      ],
      "metadata": {
        "id": "XNqLwsvHSIpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lyLvYGsvXYlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvis"
      ],
      "metadata": {
        "id": "sv3tyfvBXZrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown This codes saves html of network but doesn't display on colab\n",
        "import nltk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "from pyvis.network import Network\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text into sentences and then into words\n",
        "sentences = sent_tokenize(text)\n",
        "tokens = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Remove punctuations and specific words\n",
        "tokens = [[word for word in sent if word not in string.punctuation and word not in [\"n't\", \"think\", \"thought\",\"used\",\"using\",\"class\",\"classes\",\"course\",\"taking\",\"'s\",\"things\"]] for sent in tokens]\n",
        "\n",
        "# Flatten the list of sentences into a list of words\n",
        "flat_tokens = [word for sent in tokens for word in sent]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(flat_tokens)\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 10  # Adjust the threshold as needed\n",
        "\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes for each filtered token\n",
        "G.add_nodes_from(filtered_tokens)\n",
        "\n",
        "# Add edges based on co-occurrence within the same sentence\n",
        "for sent in tokens:\n",
        "    for i in range(len(sent) - 1):\n",
        "        if sent[i] in filtered_tokens and sent[i+1] in filtered_tokens:\n",
        "            G.add_edge(sent[i], sent[i+1])\n",
        "\n",
        "# Create a Network instance\n",
        "nt = Network(notebook=True, height=\"500px\", width=\"100%\", cdn_resources='remote')\n",
        "\n",
        "# Add nodes and edges to the network\n",
        "for node in G.nodes:\n",
        "    nt.add_node(node)\n",
        "for edge in G.edges:\n",
        "    nt.add_edge(edge[0], edge[1])\n",
        "\n",
        "# Visualize the network graph\n",
        "nt.show(\"network_graph.html\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TG8Hni-LXTcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust to Ipython for display"
      ],
      "metadata": {
        "id": "KqTBxZx6ZqKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Network graph (works well)\n",
        "import nltk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "from pyvis.network import Network\n",
        "from IPython.display import HTML\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text into sentences and then into words\n",
        "sentences = sent_tokenize(text)\n",
        "tokens = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Convert tokens to lowercase\n",
        "tokens = [[word.lower() for word in sent if word not in string.punctuation and word not in [\"n't\", \"think\", \"thought\", \"used\", \"using\", \"class\", \"classes\", \"course\", \"taking\", \"'s\", \"things\"]] for sent in tokens]\n",
        "\n",
        "#\n",
        "\n",
        "# Remove punctuations and specific words\n",
        "# tokens = [[word for word in sent if word not in string.punctuation and word not in [\"n't\", \"think\", \"thought\",\"used\",\"using\",\"class\",\"classes\",\"course\",\"taking\",\"'s\",\"things\"]] for sent in tokens]\n",
        "tokens = [[word for word in sent if word not in string.punctuation] for sent in tokens]\n",
        "# Flatten the list of sentences into a list of words\n",
        "flat_tokens = [word for sent in tokens for word in sent]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(flat_tokens)\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = input('Type threshold: e.g., 5, 10')  # Adjust the threshold as needed\n",
        "threshold = int(threshold)\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes for each filtered token\n",
        "G.add_nodes_from(filtered_tokens)\n",
        "\n",
        "# Add edges based on co-occurrence within the same sentence\n",
        "for sent in tokens:\n",
        "    for i in range(len(sent) - 1):\n",
        "        if sent[i] in filtered_tokens and sent[i+1] in filtered_tokens:\n",
        "            G.add_edge(sent[i], sent[i+1])\n",
        "\n",
        "# nt.set_options(options)\n",
        "\n",
        "# Add nodes and edges to the network\n",
        "for node in G.nodes:\n",
        "    nt.add_node(node)\n",
        "for edge in G.edges:\n",
        "    nt.add_edge(edge[0], edge[1])\n",
        "\n",
        "# Visualize the network graph and save it to an HTML file\n",
        "nt.save_graph(\"network_graph.html\")\n",
        "\n",
        "# Display the HTML file in Colab\n",
        "display(HTML(\"network_graph.html\"))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mUmsiZYtZseB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting optimal threshold: using frequency density calculation"
      ],
      "metadata": {
        "id": "5-Ozd_7FsamV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Density calculation and graph:\n",
        "import nltk\n",
        "import networkx as nx\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text into sentences and then into words\n",
        "sentences = sent_tokenize(text)\n",
        "tokens = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Initialize WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize tokens\n",
        "lemmatized_tokens = [[lemmatizer.lemmatize(word) for word in sent] for sent in tokens]\n",
        "\n",
        "# Convert tokens to lowercase and remove punctuation\n",
        "lemmatized_tokens = [[word.lower() for word in sent if word not in string.punctuation] for sent in lemmatized_tokens]\n",
        "\n",
        "# Flatten the list of sentences into a list of words\n",
        "flat_lemmatized_tokens = [word for sent in lemmatized_tokens for word in sent]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(flat_lemmatized_tokens)\n",
        "\n",
        "# Calculate the total number of possible edges (in a complete graph)\n",
        "total_possible_edges = len(set(flat_lemmatized_tokens)) * (len(set(flat_lemmatized_tokens)) - 1)\n",
        "\n",
        "# Define a function to calculate the density of the graph\n",
        "def calculate_density(threshold, word_freq):\n",
        "    high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "    G = nx.DiGraph()\n",
        "    G.add_nodes_from(high_freq_words)\n",
        "    for sent in lemmatized_tokens:\n",
        "        for i in range(len(sent) - 1):\n",
        "            if sent[i] in high_freq_words and sent[i+1] in high_freq_words:\n",
        "                G.add_edge(sent[i], sent[i+1])\n",
        "    current_density = nx.density(G)\n",
        "    return current_density\n",
        "\n",
        "# Define your target density here\n",
        "target_density = 0.1  # for example, 10%\n",
        "\n",
        "# Initialize threshold to a value that will be updated\n",
        "threshold = 2  # starting with a low threshold\n",
        "current_density = calculate_density(threshold, word_freq)\n",
        "\n",
        "# Adjust threshold until the density is close to the target density\n",
        "while current_density < target_density and threshold < max(word_freq.values()):\n",
        "    threshold += 1\n",
        "    current_density = calculate_density(threshold, word_freq)\n",
        "\n",
        "print(f\"Optimal threshold: {threshold}\")\n",
        "print(f\"Resulting density: {current_density}\")\n",
        "\n",
        "# Calculate densities for a range of thresholds\n",
        "thresholds = range(2, max(word_freq.values()) + 1)\n",
        "densities = [calculate_density(threshold, word_freq) for threshold in thresholds]\n",
        "\n",
        "# Find the threshold closest to the target density\n",
        "# and its corresponding index\n",
        "closest_threshold = min(thresholds, key=lambda t: abs(calculate_density(t, word_freq) - target_density))\n",
        "index_of_closest_threshold = thresholds.index(closest_threshold)\n",
        "\n",
        "# Plot the density graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(thresholds, densities, marker='o')\n",
        "plt.axhline(y=target_density, color='r', linestyle='--', label=f'Target Density ({target_density})')\n",
        "plt.axvline(x=closest_threshold, color='g', linestyle='--', label=f'Closest Threshold ({closest_threshold})')\n",
        "plt.title('Graph Density vs. Word Frequency Threshold')\n",
        "plt.xlabel('Word Frequency Threshold')\n",
        "plt.ylabel('Graph Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('density_graph.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TZ0iP_mEokf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# network analysis with lemmatized tokens (final version?)"
      ],
      "metadata": {
        "id": "RPdUfJxPfe1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Network graph (final version?): threshold set to 11\n",
        "import nltk\n",
        "import networkx as nx\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import string\n",
        "from pyvis.network import Network\n",
        "from IPython.display import HTML\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example text\n",
        "text = ' '.join(df['Essay'].dropna().astype(str))\n",
        "\n",
        "# Tokenize the text into sentences and then into words\n",
        "sentences = sent_tokenize(text)\n",
        "tokens = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Initialize WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize tokens\n",
        "lemmatized_tokens = [[lemmatizer.lemmatize(word) for word in sent] for sent in tokens]\n",
        "\n",
        "# Convert tokens to lowercase and remove punctuation\n",
        "lemmatized_tokens = [[word.lower() for word in sent if word not in string.punctuation] for sent in lemmatized_tokens]\n",
        "\n",
        "# Flatten the list of sentences into a list of words\n",
        "flat_lemmatized_tokens = [word for sent in lemmatized_tokens for word in sent]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(flat_lemmatized_tokens)\n",
        "\n",
        "# Set a threshold for high-frequency words\n",
        "threshold = 11  # Adjust the threshold as needed\n",
        "\n",
        "# Get the high-frequency words\n",
        "high_freq_words = [word for word, freq in word_freq.items() if freq >= threshold]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in high_freq_words if word.lower() not in stop_words]\n",
        "\n",
        "# Build the network graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes for each filtered token\n",
        "G.add_nodes_from(filtered_tokens)\n",
        "\n",
        "# Add edges based on co-occurrence within the same sentence\n",
        "for sent in lemmatized_tokens:\n",
        "    for i in range(len(sent) - 1):\n",
        "        if sent[i] in filtered_tokens and sent[i+1] in filtered_tokens:\n",
        "            G.add_edge(sent[i], sent[i+1])\n",
        "\n",
        "# Create a Network instance\n",
        "nt = Network(notebook=True, height=\"800px\", width=\"100%\", cdn_resources='remote')\n",
        "\n",
        "# Add nodes and edges to the network\n",
        "for node in G.nodes:\n",
        "    nt.add_node(node)\n",
        "for edge in G.edges:\n",
        "    nt.add_edge(edge[0], edge[1])\n",
        "\n",
        "# Visualize the network graph and save it to an HTML file\n",
        "nt.save_graph(\"network_graph0.html\")\n",
        "\n",
        "# Display the HTML file in Colab\n",
        "display(HTML(\"network_graph0.html\"))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Zti2fXBEfedm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_nodes = G.number_of_nodes()\n",
        "print(number_of_nodes)\n"
      ],
      "metadata": {
        "id": "zeC7-09XmcQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the threshold using density calculation"
      ],
      "metadata": {
        "id": "DjnZNF6aohSq"
      }
    }
  ]
}