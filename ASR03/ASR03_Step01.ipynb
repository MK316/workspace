{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNiSW56yF0sKFK4rRF/FrtA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/workspace/blob/main/ASR03/ASR03_Step01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [0] Setting: openai install"
      ],
      "metadata": {
        "id": "5rlcomSRlGXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8_RTHFIUhly"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = input('openai API key here')"
      ],
      "metadata": {
        "id": "xyXTUe1sUmyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1] Google speech file to read and data summary\n",
        "Jump to [1A]"
      ],
      "metadata": {
        "id": "txu8Xkdpq-oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ASR03_info.csv: 35 files with info\n",
        "\n",
        "|ID |\tL1 |\tL3|\tAge|\tL2Onset|\tStayOfPurpose|\tEngResidence|\tLOR |\tTarget|\n",
        "|--|--|--|--|--|--|--|--|--|"
      ],
      "metadata": {
        "id": "Xk53DxXHsvD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/workspace/main/ASR03/data/ASR03_info.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "iW-bk7a-rWRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Add a column 'Filename' korean03.mp3\n",
        "file = df['L1']\n",
        "nameid = df['ID']\n",
        "\n",
        "newfilename = []\n",
        "\n",
        "for i in range(0, len(file)):\n",
        "  id = str(nameid[i])\n",
        "  # if len(id) == 1:\n",
        "  #   newid = \"0\" + id\n",
        "  # else:\n",
        "  #   newid = id\n",
        "  new = file[i].strip() + id+\".mp3\"\n",
        "  newfilename.append(new)\n",
        "\n",
        "print(newfilename)\n",
        "df['Filename'] = newfilename\n",
        "df.head()"
      ],
      "metadata": {
        "id": "S7w-a_1UwPQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('ASR03_info3.csv', index=False)"
      ],
      "metadata": {
        "id": "a816jppj0Jq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1A] Proceed with the new ASR03_info3 file"
      ],
      "metadata": {
        "id": "uv5jX63P0hhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ASR03_info2.csv: 35 files with info + filename is added\n",
        "\n",
        "|ID |\tL1 |\tL3|\tAge|\tL2Onset|\tStayOfPurpose|\tEngResidence|\tLOR |\tTarget|Filename|\n",
        "|--|--|--|--|--|--|--|--|--|--|"
      ],
      "metadata": {
        "id": "4kUtU5nP0X8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/workspace/main/ASR03/data/ASR03_info3.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "vIq25Ili0f70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [2] Reading audio files from Google Drive (zip file)"
      ],
      "metadata": {
        "id": "Enw32YlPuMtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2A] Audio files from Google to Colab 'myaudio' directory"
      ],
      "metadata": {
        "id": "UuD_UqQ20_94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@markdown ðŸŽ¯Mount Google drive and list files (e.g., \"asrdata\" folder in my case)\n",
        "from google.colab import drive \n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "mydir = input(\"Type the file directory in your google drive: e.g., asrdata  \")\n",
        "!ls \"/content/drive/MyDrive/{mydir}\"\n",
        "!pwd\n",
        "     "
      ],
      "metadata": {
        "cellView": "form",
        "id": "yijC2e-9uR7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸŽ¯ To do  \n",
        "\n",
        "#@markdown   [1] Make a new folder: type a new folder name (e.g., myaudio)\n",
        "import os\n",
        "\n",
        "folder_name = input(\"Type a name for the new folder.\")\n",
        "\n",
        "if not os.path.exists(folder_name):\n",
        "  os.makedirs(folder_name)\n",
        "  print(f\"A new folder name '{folder_name}' has been created.\")\n",
        "else:\n",
        "  print(f\"{folder_name} already exists.\")\n",
        "\n",
        "#@markdown [2] Unzip files: type a zip file name (e.g., SE.zip)\n",
        "\n",
        "zipname = input(\"Type your zip file name (e.g., se.zip) to process (unzip and save them under the new folder\")\n",
        "!unzip \"/content/drive/MyDrive/asrdata/{zipname}\" -d \"/content/{folder_name}/\"\n",
        "\n",
        "print(f\"Your {zipname} is unzipped under '{folder_name}' folder\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "38H6TQTjulb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸŽ¯ Unmount Google drive (clearing): optional\n",
        "\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Jwq6gsDCuvNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2B] Openai"
      ],
      "metadata": {
        "id": "kr29nLY108Rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "Een-ZJo0u8gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = input('openai API key here')"
      ],
      "metadata": {
        "id": "4dUUb0-9u2PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: Reading audio from github\n",
        "```\n",
        "import os\n",
        "url = \"https://raw.githubusercontent.com/MK316/workspace/main/ASR01/audio/F01_post.wav\"\n",
        "os.system(\"curl \" + url + \" > test.wav\")\n",
        "```"
      ],
      "metadata": {
        "id": "AakChrHd1Lie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading audio from colab"
      ],
      "metadata": {
        "id": "tiVtwTci1WOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: file list of a colab directory\n",
        "```\n",
        "import os\n",
        "path = '/content/myaudio/'\n",
        "files = os.listdir(path)\n",
        "\n",
        "for file in files:\n",
        "    print(file)\n",
        "```"
      ],
      "metadata": {
        "id": "CpKnkN4v1csk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A single file process to check\n",
        "file = df['Filename'][0]\n",
        "audio_file = open(file, \"rb\")\n",
        "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "print(transcript['text'])"
      ],
      "metadata": {
        "id": "QbdTMG6J3GpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸŽ¯Install {autotime} to measure runtime (From here, runtime appears automatically.)\n",
        "%%capture\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "cellView": "form",
        "id": "d-etxSl04sZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/myaudio')\n",
        "\n",
        "recog = []\n",
        "\n",
        "for i in range(0, len(df['ID'])):\n",
        "  file = df['Filename'][i]\n",
        "  audio_file = open(file, \"rb\")\n",
        "  transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "  recog.append(transcript['text'])\n",
        "print(recog)"
      ],
      "metadata": {
        "id": "1XXxOGsDvrie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mY9PN-C_4J1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Recognized'] = recog\n",
        "df.head()"
      ],
      "metadata": {
        "id": "UEm7Y7jO3v5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison between target and recognized\n",
        "se = df"
      ],
      "metadata": {
        "id": "2doFjvFy5HUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Calculate recognition rate, record missing words => dataframe (df2) \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# number of words in the sentence\n",
        "nws = []\n",
        "# number of words in the recognized text\n",
        "nwr = []\n",
        "# number of missing words \n",
        "nmw = []\n",
        "# number of words actually recognized\n",
        "nwar = []\n",
        "\n",
        "# Recgonition Rate\n",
        "rr = []\n",
        "#\n",
        "nr = []\n",
        "# Missing word list\n",
        "mw = []\n",
        "# Correctly recognized word list\n",
        "recword=[]\n",
        "\n",
        "\n",
        "for i in range(0,len(se['Target'])):\n",
        "  t1 = se['Target'][i]\n",
        "\n",
        "# text 1\n",
        "  txt1 = t1.lower()\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  wlist = tokenizer.tokenize(txt1)\n",
        "  nt = len(wlist)\n",
        "  nws.append(nt)\n",
        "\n",
        "# text 2\n",
        "  t2 = se['Recognized'][i]\n",
        "  txt2 = t2.lower()\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  wlist1 = tokenizer.tokenize(txt2)\n",
        "  nt1 = len(wlist1)\n",
        "  nwr.append(nt1)\n",
        "\n",
        "# Recognition rate\n",
        "\n",
        "  # from tables.idxutils import calc_chunksize\n",
        "  # from nltk.downloader import ErrorMessage\n",
        "# txt1(original text), txt2 (recognized text)\n",
        " \n",
        "  mword = []\n",
        "  rword = []\n",
        "  score = 0\n",
        "  for i in range(0, len(wlist1)):\n",
        "      w = wlist1[i]\n",
        "\n",
        "      if w in wlist:\n",
        "        sc = 1\n",
        "        rword.append(w)\n",
        "      else:\n",
        "        sc = 0\n",
        "        mword.append(w)\n",
        "\n",
        "      score = score + sc\n",
        "      mwords = ', '.join(mword)\n",
        "      rwords = ', '.join(rword)\n",
        "  mw.append(mwords)\n",
        "  missingword = round((score/len(wlist))*100,2)\n",
        "  nr.append(score)\n",
        "  recword.append(rwords)\n",
        "\n",
        "  # RecRate = float(format(missingword, '.2f'))\n",
        "  # ErrRate = float(format((100.0 - RecRate), '.2f'))\n",
        "  \n",
        "  rr.append(missingword)\n",
        "  # print('Matching words: %d'%score, 'out of %d words'%len(wlist))\n",
        "  # print(\"=\"*50)\n",
        "  # print('Recognition Rate: %f %%'%RecRate)\n",
        "  # print('Error Rate: %f %%'%ErrRate)\n",
        "\n",
        "se['LenS'] = nws\n",
        "se['LenR'] = nwr\n",
        "se['N_RecW'] = nr\n",
        "se['RecRate'] = rr\n",
        "se['Recognized_words'] = recword\n",
        "se['MissRec_words'] = mw\n",
        "\n",
        "se.head(); se.tail()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6vtdPOKE4jBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "se.describe()"
      ],
      "metadata": {
        "id": "MoCHLPzD574S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = se\n",
        "df1.to_csv(\"ASR03_info_recognized.csv\", index=False)"
      ],
      "metadata": {
        "id": "eAQdApun6Fwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’œ From here: df (with recognized words and texts)"
      ],
      "metadata": {
        "id": "CC6DnnyN6XfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/workspace/main/ASR03/data/ASR03_info_recognized.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Z-jzhT7r6trC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df[['Filename', 'Age', 'L2Onset','LOR','LenR','N_RecW','RecRate']]"
      ],
      "metadata": {
        "id": "boqxu9-Z7R9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WER"
      ],
      "metadata": {
        "id": "_kDgoAul8f0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown + Install and import libraries\n",
        "%%capture\n",
        "!pip install Levenshtein\n",
        "import Levenshtein as lev\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "B4QhHVhn8IMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown + Calculate WER and WER_lev and add columns to df\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import Levenshtein as lev\n",
        "#@markdown + Column names 'Original' and 'Recognized' for comparison\n",
        "def calculate_wer_word_level(s1, s2):\n",
        "    \"\"\"\n",
        "    Calculation of WER with Dynamic Time Warping algorithm at word level.\n",
        "    \"\"\"\n",
        "\n",
        "    # splitting the sentences into words\n",
        "    s1_words = s1.split()\n",
        "    s2_words = s2.split()\n",
        "\n",
        "    # preparing the matrix for the Dynamic Time Warping algorithm\n",
        "    dtw_matrix = np.zeros((len(s1_words)+1, len(s2_words)+1))\n",
        "    for i in range(len(s1_words)+1):\n",
        "        for j in range(len(s2_words)+1):\n",
        "            if i == 0:\n",
        "                dtw_matrix[0][j] = j\n",
        "            elif j == 0:\n",
        "                dtw_matrix[i][0] = i\n",
        "\n",
        "    # Dynamic Time Warping algorithm\n",
        "    for i in range(1, len(s1_words)+1):\n",
        "        for j in range(1, len(s2_words)+1):\n",
        "            cost = 0 if s1_words[i-1] == s2_words[j-1] else 1\n",
        "            dtw_matrix[i][j] = cost + min([dtw_matrix[i-1][j], dtw_matrix[i][j-1], dtw_matrix[i-1][j-1]])\n",
        "\n",
        "    wer = dtw_matrix[len(s1_words)][len(s2_words)] / len(s1_words)\n",
        "\n",
        "    return wer\n",
        "\n",
        "def calculate_wer_lev(s1, s2):\n",
        "    \"\"\"\n",
        "    Calculation of WER with Levenshtein distance at character level.\n",
        "    \"\"\"\n",
        "\n",
        "    # splitting the sentences into words\n",
        "    s1_words = s1.split()\n",
        "    s2_words = s2.split()\n",
        "\n",
        "    # calculating Levenshtein distance\n",
        "    edit_distance = lev.distance(' '.join(s1_words), ' '.join(s2_words))\n",
        "\n",
        "    # normalizing by the length of the original sentence (s1)\n",
        "    wer = edit_distance / len(s1_words)\n",
        "\n",
        "    return wer\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5HHOYQA48iTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame and it has columns 'Sentence' and 'Recognized'\n",
        "import numpy as np\n",
        "\n",
        "wer_values_word_level = df.apply(lambda row: calculate_wer_word_level(row['Target'], row['Recognized']), axis=1)\n",
        "wer_values_lev = df.apply(lambda row: calculate_wer_lev(row['Target'], row['Recognized']), axis=1)\n",
        "\n",
        "# Adding WER values as new columns to the DataFrame\n",
        "df['WER'] = wer_values_word_level\n",
        "df['WER_lev'] = wer_values_lev"
      ],
      "metadata": {
        "id": "p_-JapQ789UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df[['Filename', 'Age', 'L2Onset','LOR','LenR','N_RecW','RecRate','WER','WER_lev']]\n",
        "df1"
      ],
      "metadata": {
        "id": "RD_0jTOj9MsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://github.com/MK316/workspace/tree/main/ASR03/audio"
      ],
      "metadata": {
        "id": "ZixNsR94-Avb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content')"
      ],
      "metadata": {
        "id": "FgndQO6T-mrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "url = \"https://raw.githubusercontent.com/MK316/workspace/main/ASR03/audio/korean30.mp3\"\n",
        "os.system(\"curl \" + url + \" > test.mp3\")\n"
      ],
      "metadata": {
        "id": "6ohLF3tH-TQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file = open('test.mp3', \"rb\")\n",
        "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "print(transcript['text'])"
      ],
      "metadata": {
        "id": "3gzatjSh-gYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('ASR03_results.csv',index=False)"
      ],
      "metadata": {
        "id": "vTCuYzSh_KHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "JH1Vn3IIAfMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Assuming df is your dataframe and it has columns 'LOR' and 'WER'\n",
        "# Create a fitted model\n",
        "model = smf.ols(formula='WER ~ LOR', data=df).fit()\n",
        "\n",
        "# Print out the statistics of the model\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "SPK81qJYAgeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down the results:\n",
        "\n",
        "R-squared: This is the coefficient of determination. It explains the proportion of the variance in the dependent variable that is predictable from the independent variable. In this case, 15.3% of the variability in WER can be explained by LOR. This isn't particularly high, which might suggest that other variables not included in the model could also be impacting WER.\n",
        "\n",
        "Adj. R-squared: This is the R-squared value that has been adjusted based on the number of predictors in the model. It is more accurate and should be used over the regular R-squared if you have more than one predictor. Since this model has only one predictor (LOR), the adjusted R-squared is very close to the regular R-squared.\n",
        "\n",
        "coef: These are the coefficients of the linear regression equation. The equation would look something like this: WER = 0.0473 + 0.0072*LOR. That means, for each unit increase in LOR, WER increases by 0.0072 units, on average, assuming all other variables are held constant.\n",
        "\n",
        "P>|t|: This is the p-value associated with the null hypothesis that the coefficient is equal to zero (no effect). A p-value of less than 0.05 is typically considered statistically significant at the 5% level. In this case, the p-value for LOR is 0.020, which is less than 0.05, suggesting that LOR has a statistically significant effect on WER.\n",
        "\n",
        "[0.025 0.975]: These are the 95% confidence intervals for the coefficients. This means we are 95% confident that the true population coefficient for LOR lies between 0.001 and 0.013.\n",
        "\n",
        "Durbin-Watson: This tests for the presence of autocorrelation (a relationship between values separated from each other by a given time lag) in the residuals. Values range from 0 to 4, and values around 2 suggest no autocorrelation. Your value of 1.890 suggests that there is no significant autocorrelation.\n",
        "\n",
        "Prob (F-statistic): This is the p-value of the overall regression model. It tests the null hypothesis that all regression coefficients are equal to zero. A p-value of less than 0.05 indicates that at least one of the predictors is significantly related to the outcome variable. In this case, the p-value is 0.0201, which is less than 0.05, indicating that the model is statistically significant.\n",
        "\n",
        "Finally, it's important to remember that correlation does not imply causation. While LOR might be associated with WER, it doesn't necessarily mean that changes in LOR cause changes in WER."
      ],
      "metadata": {
        "id": "2lB1UofiCC2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Regression line plot\n",
        "sns.regplot(x='LOR', y='WER', data=df)\n",
        "plt.show()\n",
        "\n",
        "# Residuals plot\n",
        "residuals = df['WER'] - model.predict()\n",
        "sns.residplot(x=model.predict(), y=residuals, lowess=True, color=\"g\")\n",
        "plt.show()\n",
        "\n",
        "# QQ plot\n",
        "import statsmodels.api as sm\n",
        "sm.qqplot(residuals, line ='45')\n",
        "plt.show()\n",
        "\n",
        "# Histogram of residuals\n",
        "plt.hist(residuals)\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix\n",
        "corrMatrix = df.corr()\n",
        "sns.heatmap(corrMatrix, annot=True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nz9wIXACAhJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few ways to visually summarize the results of your regression analysis:\n",
        "\n",
        "Scatter plot with fitted line (Regression line plot): You can create a scatter plot of the data and add the line of best fit from your model. This will provide a visualization of how well the line fits the data. You can also plot the residuals (the differences between the observed and predicted values) to see how evenly they're distributed around the line.\n",
        "\n",
        "Residuals plot: A residuals plot shows the difference between the observed and predicted values (the residuals) for each observation, typically plotted against the predicted values. If your model is a good fit, the points in the residuals plot should be randomly and evenly dispersed around zero.\n",
        "\n",
        "QQ Plot: A Quantile-Quantile plot, or QQ plot, is used to check if your data follows a particular distribution. In the case of linear regression, we often want to check if the residuals follow a normal distribution. If the points roughly fall along the diagonal line, then the residuals are normally distributed.\n",
        "\n",
        "Histogram of residuals: This can also be used to check if residuals are normally distributed.\n",
        "\n",
        "Correlation matrix: While not directly related to the regression results, a correlation matrix can give you an overview of how all your variables relate to each other.\n",
        "\n",
        "Here is how you can plot some of these in Python:"
      ],
      "metadata": {
        "id": "fyWUWpB1CHZ9"
      }
    }
  ]
}