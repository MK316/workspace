{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPHzKolECw03MlZmFcfuuFm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/workspace/blob/main/Kangkim23/Step1_dataprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1] Setting: openai install"
      ],
      "metadata": {
        "id": "5rlcomSRlGXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8_RTHFIUhly"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = input('openai API key here')"
      ],
      "metadata": {
        "id": "xyXTUe1sUmyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2] Google speech file to read and data summary\n",
        "Jump to [1A]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u8guonS-qmaD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfJVMAfKpdGQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/workspace/main/Kangkim23/koreanaccent_log.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Add a column 'Filename' korean03.mp3\n",
        "file = df['L1']\n",
        "nameid = df['Fileid']\n",
        "\n",
        "newfilename = []\n",
        "\n",
        "for i in range(0, len(file)):\n",
        "  id = str(nameid[i])\n",
        "  # if len(id) == 1:\n",
        "  #   newid = \"0\" + id\n",
        "  # else:\n",
        "  #   newid = id\n",
        "  new = file[i].strip() + id+\".mp3\"\n",
        "  newfilename.append(new)\n",
        "\n",
        "print(newfilename)\n",
        "df['Filename'] = newfilename\n",
        "df.head()"
      ],
      "metadata": {
        "id": "oTTroAMSqylT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('koreanaccent_info3.csv', index=False)"
      ],
      "metadata": {
        "id": "lBxsRk0hq1Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2A] Audio files from Google to Colab 'myaudio' directory"
      ],
      "metadata": {
        "id": "tT8CtveVq9YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "QxXmBfNaRYIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@markdown ðŸŽ¯Mount Google drive and list files (e.g., \"asrdata\" folder in my case)\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "mydir = input(\"Type the file directory in your google drive: e.g., asrdata  \")\n",
        "!ls \"/content/drive/MyDrive/{mydir}\"\n",
        "!pwd\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1RFNA6xrq9r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Specify zip file folder name on Google"
      ],
      "metadata": {
        "id": "ZbLEr40ONLdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸŽ¯ To do\n",
        "\n",
        "#@markdown   [1] Make a new folder: type a new folder name (e.g., myaudio)\n",
        "import os\n",
        "\n",
        "folder_name = input(\"Type a name for the new folder.\")\n",
        "\n",
        "if not os.path.exists(folder_name):\n",
        "  os.makedirs(folder_name)\n",
        "  print(f\"A new folder name '{folder_name}' has been created.\")\n",
        "else:\n",
        "  print(f\"{folder_name} already exists.\")\n",
        "\n",
        "#@markdown [2] Unzip files: type a zip file name (e.g., SE.zip)\n",
        "\n",
        "zipname = input(\"Type your zip file name (e.g., se.zip) to process (unzip and save them under the new folder\")\n",
        "!unzip \"/content/drive/MyDrive/accentdata/{zipname}\" -d \"/content/{folder_name}/\"\n",
        "\n",
        "print(f\"Your {zipname} is unzipped under '{folder_name}' folder\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uS2LCtgorBck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸŽ¯ Unmount Google drive (clearing): optional\n",
        "\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "Wddl0NZkrD0s",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1] Setting: openai install"
      ],
      "metadata": {
        "id": "K5eP0uxSrInf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbAGV9NArInf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = input('openai API key here')"
      ],
      "metadata": {
        "id": "mlPL0mlorInf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: Reading audio from github\n",
        "```\n",
        "import os\n",
        "url = \"https://raw.githubusercontent.com/MK316/workspace/main/ASR01/audio/F01_post.wav\"\n",
        "os.system(\"curl \" + url + \" > test.wav\")\n",
        "```"
      ],
      "metadata": {
        "id": "AakChrHd1Lie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: file list of a colab directory\n",
        "```\n",
        "import os\n",
        "path = '/content/myaudio/'\n",
        "files = os.listdir(path)\n",
        "\n",
        "for file in files:\n",
        "    print(file)\n",
        "```"
      ],
      "metadata": {
        "id": "CpKnkN4v1csk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/myaudio\")"
      ],
      "metadata": {
        "id": "QAJ8FWW_VMNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A single file process to check\n",
        "file = df['Filename'][10]\n",
        "audio_file = open(file, \"rb\")\n",
        "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "print(transcript['text'])"
      ],
      "metadata": {
        "id": "QbdTMG6J3GpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = df['Filename'][10]\n",
        "audio_file = open(file, \"rb\")\n",
        "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file, language=\"en\")\n",
        "print(transcript['text'])\n"
      ],
      "metadata": {
        "id": "M8RuFtARVpQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸŽ¯Install {autotime} to measure runtime (From here, runtime appears automatically.)\n",
        "%%capture\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "id": "TfUlEoc8rSgW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.chdir('/content/myaudio')\n",
        "!pwd"
      ],
      "metadata": {
        "id": "klhbgkT4RiKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recog = []\n",
        "\n",
        "for i in range(0, len(df['Fileid'])):\n",
        "  file = df['Filename'][i]\n",
        "  audio_file = open(file, \"rb\")\n",
        "  transcript = openai.Audio.transcribe(\"whisper-1\", audio_file, language=\"en\")\n",
        "  recog.append(transcript['text'])\n",
        "print(recog)"
      ],
      "metadata": {
        "id": "PW51VcW7rTgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Recognized'] = recog\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8qcfBZkBrU45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison between target and recognized\n",
        "se = df"
      ],
      "metadata": {
        "id": "snHxfgFlrV0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Calculate recognition rate, record missing words => dataframe (df2)\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# number of words in the sentence\n",
        "nws = []\n",
        "# number of words in the recognized text\n",
        "nwr = []\n",
        "# number of missing words\n",
        "nmw = []\n",
        "# number of words actually recognized\n",
        "nwar = []\n",
        "\n",
        "# Recgonition Rate\n",
        "rr = []\n",
        "#\n",
        "nr = []\n",
        "# Missing word list\n",
        "mw = []\n",
        "# Correctly recognized word list\n",
        "recword=[]\n",
        "\n",
        "\n",
        "for i in range(0,len(se['Target'])):\n",
        "  t1 = se['Target'][i]\n",
        "\n",
        "# text 1\n",
        "  txt1 = t1.lower()\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  wlist = tokenizer.tokenize(txt1)\n",
        "  nt = len(wlist)\n",
        "  nws.append(nt)\n",
        "\n",
        "# text 2\n",
        "  t2 = se['Recognized'][i]\n",
        "  txt2 = t2.lower()\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  wlist1 = tokenizer.tokenize(txt2)\n",
        "  nt1 = len(wlist1)\n",
        "  nwr.append(nt1)\n",
        "\n",
        "# Recognition rate\n",
        "\n",
        "  # from tables.idxutils import calc_chunksize\n",
        "  # from nltk.downloader import ErrorMessage\n",
        "# txt1(original text), txt2 (recognized text)\n",
        "\n",
        "  mword = []\n",
        "  rword = []\n",
        "  score = 0\n",
        "  for i in range(0, len(wlist1)):\n",
        "      w = wlist1[i]\n",
        "\n",
        "      if w in wlist:\n",
        "        sc = 1\n",
        "        rword.append(w)\n",
        "      else:\n",
        "        sc = 0\n",
        "        mword.append(w)\n",
        "\n",
        "      score = score + sc\n",
        "      mwords = ', '.join(mword)\n",
        "      rwords = ', '.join(rword)\n",
        "  mw.append(mwords)\n",
        "  missingword = round((score/len(wlist))*100,2)\n",
        "  nr.append(score)\n",
        "  recword.append(rwords)\n",
        "\n",
        "  # RecRate = float(format(missingword, '.2f'))\n",
        "  # ErrRate = float(format((100.0 - RecRate), '.2f'))\n",
        "\n",
        "  rr.append(missingword)\n",
        "  # print('Matching words: %d'%score, 'out of %d words'%len(wlist))\n",
        "  # print(\"=\"*50)\n",
        "  # print('Recognition Rate: %f %%'%RecRate)\n",
        "  # print('Error Rate: %f %%'%ErrRate)\n",
        "\n",
        "se['LenS'] = nws\n",
        "se['LenR'] = nwr\n",
        "se['N_RecW'] = nr\n",
        "se['RecRate'] = rr\n",
        "se['Recognized_words'] = recword\n",
        "se['MissRec_words'] = mw\n",
        "\n",
        "se.head(); se.tail()"
      ],
      "metadata": {
        "id": "mju90NI1rXMo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "se.describe()"
      ],
      "metadata": {
        "id": "xUQxzqd0rYpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content')"
      ],
      "metadata": {
        "id": "yJxCXryVrdgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = se\n",
        "df1.to_csv(\"koreanaccent_recognized.csv\", index=False)"
      ],
      "metadata": {
        "id": "eAQdApun6Fwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’œ From here: df (with recognized words and texts)"
      ],
      "metadata": {
        "id": "CC6DnnyN6XfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/workspace/main/Kangkim23/koreanaccent_recognized.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Z-jzhT7r6trC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df[['Filename', 'Age', 'L2Onset','LOR','LenR','N_RecW','RecRate']]"
      ],
      "metadata": {
        "id": "boqxu9-Z7R9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WER"
      ],
      "metadata": {
        "id": "_kDgoAul8f0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown + Install and import libraries\n",
        "%%capture\n",
        "!pip install Levenshtein\n",
        "import Levenshtein as lev\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "B4QhHVhn8IMc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown + Calculate WER and WER_lev and add columns to df\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import Levenshtein as lev\n",
        "#@markdown + Column names 'Original' and 'Recognized' for comparison\n",
        "def calculate_wer_word_level(s1, s2):\n",
        "    \"\"\"\n",
        "    Calculation of WER with Dynamic Time Warping algorithm at word level.\n",
        "    \"\"\"\n",
        "\n",
        "    # splitting the sentences into words\n",
        "    s1_words = s1.split()\n",
        "    s2_words = s2.split()\n",
        "\n",
        "    # preparing the matrix for the Dynamic Time Warping algorithm\n",
        "    dtw_matrix = np.zeros((len(s1_words)+1, len(s2_words)+1))\n",
        "    for i in range(len(s1_words)+1):\n",
        "        for j in range(len(s2_words)+1):\n",
        "            if i == 0:\n",
        "                dtw_matrix[0][j] = j\n",
        "            elif j == 0:\n",
        "                dtw_matrix[i][0] = i\n",
        "\n",
        "    # Dynamic Time Warping algorithm\n",
        "    for i in range(1, len(s1_words)+1):\n",
        "        for j in range(1, len(s2_words)+1):\n",
        "            cost = 0 if s1_words[i-1] == s2_words[j-1] else 1\n",
        "            dtw_matrix[i][j] = cost + min([dtw_matrix[i-1][j], dtw_matrix[i][j-1], dtw_matrix[i-1][j-1]])\n",
        "\n",
        "    wer = dtw_matrix[len(s1_words)][len(s2_words)] / len(s1_words)\n",
        "\n",
        "    return wer\n",
        "\n",
        "def calculate_wer_lev(s1, s2):\n",
        "    \"\"\"\n",
        "    Calculation of WER with Levenshtein distance at character level.\n",
        "    \"\"\"\n",
        "\n",
        "    # splitting the sentences into words\n",
        "    s1_words = s1.split()\n",
        "    s2_words = s2.split()\n",
        "\n",
        "    # calculating Levenshtein distance\n",
        "    edit_distance = lev.distance(' '.join(s1_words), ' '.join(s2_words))\n",
        "\n",
        "    # normalizing by the length of the original sentence (s1)\n",
        "    wer = edit_distance / len(s1_words)\n",
        "\n",
        "    return wer\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5HHOYQA48iTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame and it has columns 'Sentence' and 'Recognized'\n",
        "import numpy as np\n",
        "\n",
        "wer_values_word_level = df.apply(lambda row: calculate_wer_word_level(row['Target'], row['Recognized']), axis=1)\n",
        "wer_values_lev = df.apply(lambda row: calculate_wer_lev(row['Target'], row['Recognized']), axis=1)\n",
        "\n",
        "# Adding WER values as new columns to the DataFrame\n",
        "df['WER'] = wer_values_word_level\n",
        "df['WER_lev'] = wer_values_lev"
      ],
      "metadata": {
        "id": "p_-JapQ789UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df[['Filename', 'Age', 'L2Onset','LOR','LenR','N_RecW','RecRate','WER','WER_lev']]\n",
        "df1"
      ],
      "metadata": {
        "id": "RD_0jTOj9MsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://github.com/MK316/workspace/tree/main/ASR03/audio"
      ],
      "metadata": {
        "id": "ZixNsR94-Avb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content')"
      ],
      "metadata": {
        "id": "FgndQO6T-mrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "url = \"https://raw.githubusercontent.com/MK316/workspace/main/ASR03/audio/korean30.mp3\"\n",
        "os.system(\"curl \" + url + \" > test.mp3\")\n"
      ],
      "metadata": {
        "id": "6ohLF3tH-TQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file = open('test.mp3', \"rb\")\n",
        "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "print(transcript['text'])"
      ],
      "metadata": {
        "id": "3gzatjSh-gYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('ASR03_results.csv',index=False)"
      ],
      "metadata": {
        "id": "vTCuYzSh_KHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}